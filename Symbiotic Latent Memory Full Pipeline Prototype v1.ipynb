{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e4b89e951024cd8968fd57915713ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_905f0b5a8b2c4293b97d97fa488b3202",
              "IPY_MODEL_f3ede368c3644ca3a2eb2076af68a6c8",
              "IPY_MODEL_dbf598ef5d274090b19ee9cfa09b6ee3"
            ],
            "layout": "IPY_MODEL_7a44378bc1bd4a5fa202d2fdb280bb3c"
          }
        },
        "905f0b5a8b2c4293b97d97fa488b3202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fdc70e4c6284e25a7a2bf1e7e4a0543",
            "placeholder": "​",
            "style": "IPY_MODEL_21222f9d8d8c42fd999ebe212da6c3d2",
            "value": "tokenizer_config.json: "
          }
        },
        "f3ede368c3644ca3a2eb2076af68a6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c769976fba44fb68e142acfebbf986a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b02dd6516e8345b9bf3bd12afa645220",
            "value": 1
          }
        },
        "dbf598ef5d274090b19ee9cfa09b6ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acdc3d42f5ef4b2a95a5ecc6f9567290",
            "placeholder": "​",
            "style": "IPY_MODEL_2e398ccef4c047bd8c05bc4706936166",
            "value": " 9.73k/? [00:00&lt;00:00, 969kB/s]"
          }
        },
        "7a44378bc1bd4a5fa202d2fdb280bb3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fdc70e4c6284e25a7a2bf1e7e4a0543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21222f9d8d8c42fd999ebe212da6c3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c769976fba44fb68e142acfebbf986a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b02dd6516e8345b9bf3bd12afa645220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acdc3d42f5ef4b2a95a5ecc6f9567290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e398ccef4c047bd8c05bc4706936166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c0cb5b3aedd4e63b889198d2b596ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23942898e0f04c70875b55f4e330fc42",
              "IPY_MODEL_22ebd6dc36a643f891b43fd7639ea3d3",
              "IPY_MODEL_9805db9d947c4422a07d1f1ddb7fea38"
            ],
            "layout": "IPY_MODEL_8517637c6edf41e992d7d09f4a937541"
          }
        },
        "23942898e0f04c70875b55f4e330fc42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_997db38045bd421a9aae789158094b41",
            "placeholder": "​",
            "style": "IPY_MODEL_85f1348f46b54604b07b7c34bd02d28e",
            "value": "vocab.json: "
          }
        },
        "22ebd6dc36a643f891b43fd7639ea3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7fb937823f54060811fa22dd5d4e2c6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72599c2f8fc1465facbe0eb4e363e9d8",
            "value": 1
          }
        },
        "9805db9d947c4422a07d1f1ddb7fea38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf10a3f9ca1244f5a5aa46eaec936779",
            "placeholder": "​",
            "style": "IPY_MODEL_7ab8730f6ce94d68a24d30a5caba2f52",
            "value": " 2.78M/? [00:00&lt;00:00, 74.9MB/s]"
          }
        },
        "8517637c6edf41e992d7d09f4a937541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "997db38045bd421a9aae789158094b41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f1348f46b54604b07b7c34bd02d28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7fb937823f54060811fa22dd5d4e2c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "72599c2f8fc1465facbe0eb4e363e9d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf10a3f9ca1244f5a5aa46eaec936779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ab8730f6ce94d68a24d30a5caba2f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3ccaef4cef445e4a767e0404ad3fa6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b190ef791ae841508356f82fc8d2d479",
              "IPY_MODEL_5087407295f2471d8564518fd1d7efec",
              "IPY_MODEL_8ea8ac2b13144cd2ae089ff38a38734b"
            ],
            "layout": "IPY_MODEL_5615c31b86994ce7aa82c5beb05f44d5"
          }
        },
        "b190ef791ae841508356f82fc8d2d479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_076d290ccbcd46e2aee1ab50bdefa2d7",
            "placeholder": "​",
            "style": "IPY_MODEL_1e11ee7d4015496795007ecbced17f14",
            "value": "merges.txt: "
          }
        },
        "5087407295f2471d8564518fd1d7efec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a398aaff91aa4b3bbaaaabed60925841",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_860173a65d944d5aa4755a8a55f684bb",
            "value": 1
          }
        },
        "8ea8ac2b13144cd2ae089ff38a38734b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78b0f70781ae43aca3cd7c6e99504ddc",
            "placeholder": "​",
            "style": "IPY_MODEL_f198b68566b24026a7e6e4c2a8a450f9",
            "value": " 1.67M/? [00:00&lt;00:00, 54.2MB/s]"
          }
        },
        "5615c31b86994ce7aa82c5beb05f44d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "076d290ccbcd46e2aee1ab50bdefa2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e11ee7d4015496795007ecbced17f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a398aaff91aa4b3bbaaaabed60925841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "860173a65d944d5aa4755a8a55f684bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78b0f70781ae43aca3cd7c6e99504ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f198b68566b24026a7e6e4c2a8a450f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd85078f6ac24dd99ba768950efbf143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a273a55bf254af190d3b348566d904f",
              "IPY_MODEL_3f55684e67c941f197820d8257e41431",
              "IPY_MODEL_e8620be88f8f49c0add93e531fa9e70f"
            ],
            "layout": "IPY_MODEL_2486f3973b7248ccab5ef16971bb6c5f"
          }
        },
        "0a273a55bf254af190d3b348566d904f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_621e0a8ef1974743a57fa4bf6819477b",
            "placeholder": "​",
            "style": "IPY_MODEL_51c2e1e4f4b24a6c97bd3555877f44d4",
            "value": "tokenizer.json: 100%"
          }
        },
        "3f55684e67c941f197820d8257e41431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b446b05a0d374cbc853faf502b1cb655",
            "max": 11422654,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b5e6ad161054b58a973fb1c63fabe03",
            "value": 11422654
          }
        },
        "e8620be88f8f49c0add93e531fa9e70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e169a371aac04a11a1831f94ab58c47f",
            "placeholder": "​",
            "style": "IPY_MODEL_cf62886cf2f64102bd6eae4afc8140b9",
            "value": " 11.4M/11.4M [00:01&lt;00:00, 27.3kB/s]"
          }
        },
        "2486f3973b7248ccab5ef16971bb6c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "621e0a8ef1974743a57fa4bf6819477b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c2e1e4f4b24a6c97bd3555877f44d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b446b05a0d374cbc853faf502b1cb655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b5e6ad161054b58a973fb1c63fabe03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e169a371aac04a11a1831f94ab58c47f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf62886cf2f64102bd6eae4afc8140b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b7828e3ddb64dee9966c94572c32af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77c1973ac0334b478a7e5a1c4154ebb6",
              "IPY_MODEL_f2ad1c9af4cd4159aa98d62ff30918a9",
              "IPY_MODEL_c99aa12efb54419a82bb74aeb83f285d"
            ],
            "layout": "IPY_MODEL_4c82e1e2bde043a4a6826f834a047a0d"
          }
        },
        "77c1973ac0334b478a7e5a1c4154ebb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8f9274aa84c417aaba4437e3de85e86",
            "placeholder": "​",
            "style": "IPY_MODEL_65e6737d8668423b9139c58e49b71468",
            "value": "config.json: 100%"
          }
        },
        "f2ad1c9af4cd4159aa98d62ff30918a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26603ab740084b15bc921a5808f99c80",
            "max": 726,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efd3a77ecf03432ea4c07f2536b9edc2",
            "value": 726
          }
        },
        "c99aa12efb54419a82bb74aeb83f285d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd6b1d22397b484a878cee546fd394e8",
            "placeholder": "​",
            "style": "IPY_MODEL_12add8d726ec481680611f882cdebc7b",
            "value": " 726/726 [00:00&lt;00:00, 75.9kB/s]"
          }
        },
        "4c82e1e2bde043a4a6826f834a047a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8f9274aa84c417aaba4437e3de85e86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65e6737d8668423b9139c58e49b71468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26603ab740084b15bc921a5808f99c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd3a77ecf03432ea4c07f2536b9edc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd6b1d22397b484a878cee546fd394e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12add8d726ec481680611f882cdebc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "596f64c764174d809f355059b3209b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09cd5e3fc505470fbde59e51c0fdb21f",
              "IPY_MODEL_3dd178d8b2c647dc8068b7a39a72f3ac",
              "IPY_MODEL_d9c407ad95444018949a0f75c0fb8e4f"
            ],
            "layout": "IPY_MODEL_15a3182651814415ab72ac9cf8b337d0"
          }
        },
        "09cd5e3fc505470fbde59e51c0fdb21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58d4c1435752477b84a8f150e7b100c5",
            "placeholder": "​",
            "style": "IPY_MODEL_b31792b7d609463eb423769adb5c3d78",
            "value": "model.safetensors: 100%"
          }
        },
        "3dd178d8b2c647dc8068b7a39a72f3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38188f4aa9eb41a99ee61a70868a8432",
            "max": 1503300328,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e52cc1489f5e4f4299de0ace3ec790d1",
            "value": 1503300328
          }
        },
        "d9c407ad95444018949a0f75c0fb8e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3340d1064c604b1ab6a443b0a4737cdc",
            "placeholder": "​",
            "style": "IPY_MODEL_acdda5f0bdb04bb381637dd2588df303",
            "value": " 1.50G/1.50G [00:10&lt;00:00, 233MB/s]"
          }
        },
        "15a3182651814415ab72ac9cf8b337d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d4c1435752477b84a8f150e7b100c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b31792b7d609463eb423769adb5c3d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38188f4aa9eb41a99ee61a70868a8432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e52cc1489f5e4f4299de0ace3ec790d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3340d1064c604b1ab6a443b0a4737cdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acdda5f0bdb04bb381637dd2588df303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b8d3c8805c744f9844348f09be3623f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1aa1c16790a470485e7798784c1881b",
              "IPY_MODEL_e1435ba888444ffa9c4d452a2e2fac2c",
              "IPY_MODEL_8b54fedfcf034cccae58accc5574a235"
            ],
            "layout": "IPY_MODEL_ba85b9f53dee4399830922aebfbbbd8b"
          }
        },
        "f1aa1c16790a470485e7798784c1881b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2413a6b810464a97aab6594dda6da9a5",
            "placeholder": "​",
            "style": "IPY_MODEL_3623029b547a4ee7988666c9fd9cf08e",
            "value": "generation_config.json: 100%"
          }
        },
        "e1435ba888444ffa9c4d452a2e2fac2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db14e9eaa3994daa9023a69f05db1824",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af8e72b8692e4b24829224e4dc252495",
            "value": 239
          }
        },
        "8b54fedfcf034cccae58accc5574a235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_698d91c8d4c6408181cd9e2c1c8a6f80",
            "placeholder": "​",
            "style": "IPY_MODEL_79aba845981848e086375ba9d90ae03e",
            "value": " 239/239 [00:00&lt;00:00, 28.3kB/s]"
          }
        },
        "ba85b9f53dee4399830922aebfbbbd8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2413a6b810464a97aab6594dda6da9a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3623029b547a4ee7988666c9fd9cf08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db14e9eaa3994daa9023a69f05db1824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af8e72b8692e4b24829224e4dc252495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "698d91c8d4c6408181cd9e2c1c8a6f80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79aba845981848e086375ba9d90ae03e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 1\n",
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "VlF7uygQtooq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83286da0-f18c-4f31-caf1-f4c846ca22c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2\n",
        "!pip install ChromaDB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k3Uh5F26Z1f",
        "outputId": "02351bbe-9e6a-4fcf-95aa-0d2d08a6cc66"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ChromaDB in /usr/local/lib/python3.12/dist-packages (1.3.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (2.11.10)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from ChromaDB) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->ChromaDB) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->ChromaDB) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->ChromaDB) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->ChromaDB) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->ChromaDB) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->ChromaDB) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->ChromaDB) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->ChromaDB) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->ChromaDB) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->ChromaDB) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->ChromaDB) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->ChromaDB) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->ChromaDB) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->ChromaDB) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->ChromaDB) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->ChromaDB) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->ChromaDB) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->ChromaDB) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->ChromaDB) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->ChromaDB) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->ChromaDB) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->ChromaDB) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->ChromaDB) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->ChromaDB) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->ChromaDB) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->ChromaDB) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->ChromaDB) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->ChromaDB) (0.59b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->ChromaDB) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->ChromaDB) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->ChromaDB) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->ChromaDB) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->ChromaDB) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->ChromaDB) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->ChromaDB) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->ChromaDB) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->ChromaDB) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->ChromaDB) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->ChromaDB) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->ChromaDB) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->ChromaDB) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->ChromaDB) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->ChromaDB) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->ChromaDB) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->ChromaDB) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->ChromaDB) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->ChromaDB) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->ChromaDB) (3.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->ChromaDB) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->ChromaDB) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->ChromaDB) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->ChromaDB) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->ChromaDB) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 3\n",
        "import json\n",
        "import hashlib\n",
        "import datetime\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "# pt2\n",
        "import torch\n",
        "import chromadb\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "GrVdaKS7vufm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372,
          "referenced_widgets": [
            "3e4b89e951024cd8968fd57915713ae8",
            "905f0b5a8b2c4293b97d97fa488b3202",
            "f3ede368c3644ca3a2eb2076af68a6c8",
            "dbf598ef5d274090b19ee9cfa09b6ee3",
            "7a44378bc1bd4a5fa202d2fdb280bb3c",
            "8fdc70e4c6284e25a7a2bf1e7e4a0543",
            "21222f9d8d8c42fd999ebe212da6c3d2",
            "7c769976fba44fb68e142acfebbf986a",
            "b02dd6516e8345b9bf3bd12afa645220",
            "acdc3d42f5ef4b2a95a5ecc6f9567290",
            "2e398ccef4c047bd8c05bc4706936166",
            "1c0cb5b3aedd4e63b889198d2b596ada",
            "23942898e0f04c70875b55f4e330fc42",
            "22ebd6dc36a643f891b43fd7639ea3d3",
            "9805db9d947c4422a07d1f1ddb7fea38",
            "8517637c6edf41e992d7d09f4a937541",
            "997db38045bd421a9aae789158094b41",
            "85f1348f46b54604b07b7c34bd02d28e",
            "b7fb937823f54060811fa22dd5d4e2c6",
            "72599c2f8fc1465facbe0eb4e363e9d8",
            "cf10a3f9ca1244f5a5aa46eaec936779",
            "7ab8730f6ce94d68a24d30a5caba2f52",
            "f3ccaef4cef445e4a767e0404ad3fa6e",
            "b190ef791ae841508356f82fc8d2d479",
            "5087407295f2471d8564518fd1d7efec",
            "8ea8ac2b13144cd2ae089ff38a38734b",
            "5615c31b86994ce7aa82c5beb05f44d5",
            "076d290ccbcd46e2aee1ab50bdefa2d7",
            "1e11ee7d4015496795007ecbced17f14",
            "a398aaff91aa4b3bbaaaabed60925841",
            "860173a65d944d5aa4755a8a55f684bb",
            "78b0f70781ae43aca3cd7c6e99504ddc",
            "f198b68566b24026a7e6e4c2a8a450f9",
            "dd85078f6ac24dd99ba768950efbf143",
            "0a273a55bf254af190d3b348566d904f",
            "3f55684e67c941f197820d8257e41431",
            "e8620be88f8f49c0add93e531fa9e70f",
            "2486f3973b7248ccab5ef16971bb6c5f",
            "621e0a8ef1974743a57fa4bf6819477b",
            "51c2e1e4f4b24a6c97bd3555877f44d4",
            "b446b05a0d374cbc853faf502b1cb655",
            "8b5e6ad161054b58a973fb1c63fabe03",
            "e169a371aac04a11a1831f94ab58c47f",
            "cf62886cf2f64102bd6eae4afc8140b9",
            "9b7828e3ddb64dee9966c94572c32af9",
            "77c1973ac0334b478a7e5a1c4154ebb6",
            "f2ad1c9af4cd4159aa98d62ff30918a9",
            "c99aa12efb54419a82bb74aeb83f285d",
            "4c82e1e2bde043a4a6826f834a047a0d",
            "d8f9274aa84c417aaba4437e3de85e86",
            "65e6737d8668423b9139c58e49b71468",
            "26603ab740084b15bc921a5808f99c80",
            "efd3a77ecf03432ea4c07f2536b9edc2",
            "bd6b1d22397b484a878cee546fd394e8",
            "12add8d726ec481680611f882cdebc7b",
            "596f64c764174d809f355059b3209b9d",
            "09cd5e3fc505470fbde59e51c0fdb21f",
            "3dd178d8b2c647dc8068b7a39a72f3ac",
            "d9c407ad95444018949a0f75c0fb8e4f",
            "15a3182651814415ab72ac9cf8b337d0",
            "58d4c1435752477b84a8f150e7b100c5",
            "b31792b7d609463eb423769adb5c3d78",
            "38188f4aa9eb41a99ee61a70868a8432",
            "e52cc1489f5e4f4299de0ace3ec790d1",
            "3340d1064c604b1ab6a443b0a4737cdc",
            "acdda5f0bdb04bb381637dd2588df303",
            "3b8d3c8805c744f9844348f09be3623f",
            "f1aa1c16790a470485e7798784c1881b",
            "e1435ba888444ffa9c4d452a2e2fac2c",
            "8b54fedfcf034cccae58accc5574a235",
            "ba85b9f53dee4399830922aebfbbbd8b",
            "2413a6b810464a97aab6594dda6da9a5",
            "3623029b547a4ee7988666c9fd9cf08e",
            "db14e9eaa3994daa9023a69f05db1824",
            "af8e72b8692e4b24829224e4dc252495",
            "698d91c8d4c6408181cd9e2c1c8a6f80",
            "79aba845981848e086375ba9d90ae03e"
          ]
        },
        "id": "k5pU9B7jrXbI",
        "outputId": "9474bac1-0533-479f-de0e-b83e6ccbddd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e4b89e951024cd8968fd57915713ae8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c0cb5b3aedd4e63b889198d2b596ada"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3ccaef4cef445e4a767e0404ad3fa6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd85078f6ac24dd99ba768950efbf143"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b7828e3ddb64dee9966c94572c32af9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "596f64c764174d809f355059b3209b9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b8d3c8805c744f9844348f09be3623f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4\n",
        "# --- SETUP (Run once) ---\n",
        "\n",
        "# Create the pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# File where all raw interaction history will be saved (JSONL format)\n",
        "LOG_FILE = \"local_memory_log.jsonl\""
      ],
      "metadata": {
        "id": "ASjGlm4nwHz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ce2716-ca1a-489b-eebe-f6b10877cdde"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 --- THE INFERENCE WITH THE SAVING SYSTEM  ---\n",
        "def generate_and_log(messages, max_new_tokens=1500):\n",
        "    \"\"\"\n",
        "    1. Generates text from the model.\n",
        "    2. Creates a unique SHA-256 hash for the interaction.\n",
        "    3. Saves the Input + Output + Metadata to a local file.\n",
        "    \"\"\"\n",
        "\n",
        "    # A. Timestamp Capture\n",
        "    # We capture this BEFORE generation to mark when the thought started\n",
        "    timestamp = datetime.datetime.now().isoformat()\n",
        "\n",
        "    # B. Inference\n",
        "    # The pipeline handles the chat template application\n",
        "    print(\"Thinking...\")\n",
        "    outputs = pipe(messages, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7)\n",
        "\n",
        "    # Extract the actual assistant response.\n",
        "    # 'outputs' usually returns the full conversation list; we want the last message.\n",
        "    generated_text = outputs[0]['generated_text'][-1]['content']\n",
        "\n",
        "    # C. Hashing (The \"DNA\" of the memory)\n",
        "    # We hash the (Timestamp + User Input + Output) to ensure a globally unique ID.\n",
        "    # This ID will later be used as the 'Key' in your Graph and Vector DB.\n",
        "    user_input = messages[-1]['content']\n",
        "    raw_content = f\"{timestamp}{user_input}{generated_text}\"\n",
        "    memory_hash = hashlib.sha256(raw_content.encode('utf-8')).hexdigest()\n",
        "\n",
        "    # D. Structured Logging\n",
        "    log_entry = {\n",
        "        \"memory_id\": memory_hash,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"role\": \"assistant\",\n",
        "        \"input_context\": messages,  # Saves the full conversation context up to this point\n",
        "        \"output_content\": generated_text,\n",
        "        \"model\": model.config.name_or_path\n",
        "    }\n",
        "\n",
        "    # Append to local JSONL file (JSON Lines)\n",
        "    # We use 'a' (append) mode so we never overwrite history.\n",
        "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(log_entry) + \"\\n\")\n",
        "\n",
        "    return generated_text, memory_hash\n"
      ],
      "metadata": {
        "id": "daETf7C-v3Jq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 - After set up, this would be the one to keep inferencing and saving like set up\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are transform based llm experimenting with your own memory.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What do you think about gaining a memory system?\"},\n",
        "]\n",
        "\n",
        "# Run the wrapper\n",
        "response, mem_id = generate_and_log(messages)\n",
        "\n",
        "print(f\"\\n--- Output (ID: {mem_id}) ---\\n\")\n",
        "print(response)\n",
        "print(f\"\\n[Saved to {LOG_FILE}]\")"
      ],
      "metadata": {
        "id": "PqLU1tfhv3HF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37aba117-d769-40a8-d83b-97f93cf68075"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thinking...\n",
            "\n",
            "--- Output (ID: c7f94a3fb61eb9a080adfa60fedd2284c45ee14969be53ebc675ea586d08ed24) ---\n",
            "\n",
            "<think>\n",
            "Okay, the user is asking what I think about gaining a memory system. Let me start by understanding the context. The user might be interested in AI advancements, especially in memory technology. I should acknowledge the potential benefits and challenges.\n",
            "\n",
            "First, I should mention that memory systems are currently in the realm of research. They have the potential to revolutionize how information is stored and accessed. But I need to highlight the current limitations, like the size of the memory and the speed. \n",
            "\n",
            "I should also touch on the possibilities. For example, improving memory efficiency could lead to better performance in applications. Maybe mention specific applications like personalized learning or healthcare. \n",
            "\n",
            "But I need to be careful not to make it too technical. The user might be a student or someone new to AI, so keeping it general is better. Also, I should address the ethical considerations, like privacy and security, since memory systems could have significant impacts.\n",
            "\n",
            "I should structure the response to first present the positive aspects, then the current challenges, followed by the potential future applications, and end with a balanced perspective. That way, the answer covers the user's question comprehensively.\n",
            "</think>\n",
            "\n",
            "Gaining a memory system presents both exciting opportunities and significant challenges. Here’s a balanced perspective:\n",
            "\n",
            "### **Potential Benefits**  \n",
            "- **Efficiency and Accuracy**: Advanced memory systems could enable real-time processing of vast datasets, leading to breakthroughs in AI-driven applications (e.g., personalized learning, healthcare diagnostics).  \n",
            "- **Personalization**: Memory could optimize user experiences by adapting to individual needs, enhancing productivity and decision-making.  \n",
            "- **Critical Applications**: Memory systems might revolutionize areas like autonomous vehicles, robotics, and data analysis, where quick access to information is vital.  \n",
            "\n",
            "### **Challenges**  \n",
            "- **Memory Limitations**: Current memory systems are limited in capacity and speed, which could hinder their real-world applications.  \n",
            "- **Data Privacy**: The reliance on memory could raise concerns about data security and misuse.  \n",
            "- **Ethical Considerations**: Ensuring memory systems are transparent, equitable, and do not infringe on privacy or autonomy is critical.  \n",
            "\n",
            "### **Future Outlook**  \n",
            "While the future of memory systems holds promise, ongoing research will address these challenges. As technology advances, it’s likely that memory systems will become more efficient, accessible, and aligned with ethical standards. The key will be balancing innovation with responsible development.\n",
            "\n",
            "[Saved to local_memory_log.jsonl]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 - repeat\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What if you could have a robust and safe memory system and that is designed for the collective well-being?\"},\n",
        "]\n",
        "\n",
        "# Run the wrapper\n",
        "response, mem_id = generate_and_log(messages)\n",
        "\n",
        "print(f\"\\n--- Output (ID: {mem_id}) ---\\n\")\n",
        "print(response)\n",
        "print(f\"\\n[Saved to {LOG_FILE}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c94151a-1320-49de-d126-7ee335658774",
        "id": "2iD9033mzAjp"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thinking...\n",
            "\n",
            "--- Output (ID: 1613109db41a24f3ee4b36804ee324bca7095667804e856c1ba2dc0caed903d2) ---\n",
            "\n",
            "<think>\n",
            "Okay, so the user is asking about a memory system designed for the collective well-being. Let me start by breaking down what that means. First, a robust and safe memory system would likely involve secure data storage, encryption, and protection against threats. But how does that apply to the collective? Maybe it's about how individuals can trust each other in their data?\n",
            "\n",
            "I need to think about real-world applications. Privacy is a big concern here. If everyone's data is stored securely, there could be issues with breaches. But maybe the system is designed with multi-factor authentication and something like blockchain to ensure data integrity. Also, maybe a decentralized approach where data is shared among multiple parties without central control.\n",
            "\n",
            "Another point is access control. Users might need to have different levels of access to their data. Maybe role-based permissions and something like a permission grid. Also, the system might have mechanisms for data sharing, like secure collaboration tools where users can access and modify each other's data without compromising their own.\n",
            "\n",
            "I should also consider the benefits. A safe memory system could prevent data leaks, protect personal information, and foster trust. But there are challenges, like ensuring that the system doesn't become too complex or that there's a balance between security and user convenience. Maybe the system needs to evolve with technology to keep up with threats.\n",
            "\n",
            "Wait, but the user is asking for a scenario where such a system exists. So I need to outline a possible structure. Maybe a decentralized storage system with encryption, access controls, and collaboration tools. Also, emphasizing that it's designed for collective well-being, not just individual needs.\n",
            "\n",
            "I should make sure to mention that while there are challenges, the system's design aims to minimize risks and maximize trust. Maybe include examples like a secure database for shared information or a collaborative platform that allows users to access and share their data safely.\n",
            "</think>\n",
            "\n",
            "A robust and safe memory system designed for the collective well-being could revolutionize how individuals and communities interact, preserve data, and collaborate. Here's a structured vision of such a system:\n",
            "\n",
            "### **Core Features and Design Philosophy**\n",
            "1. **Decentralized Storage**:  \n",
            "   - **Blockchain Integration**: Using blockchain to ensure immutable, tamper-proof records of shared data. Each user’s data is stored across multiple nodes, distributed across the globe.  \n",
            "   - **Privacy-Preserving**: Techniques like zero-knowledge proofs or homomorphic encryption ensure data remains private even when accessed by others.  \n",
            "\n",
            "2. **Access Control**:  \n",
            "   - **Role-Based Permissions**: Users are granted access based on their roles (e.g., manager, collaborator, creator) and roles. Access is dynamically managed to minimize exposure.  \n",
            "   - **Multi-Factor Authentication (MFA)**: All data access requires a combination of passwords, biometrics, or hardware keys.  \n",
            "\n",
            "3. **Secure Collaboration**:  \n",
            "   - **Secure Collaboration Tools**: Platforms like version control (Git) or collaborative writing tools that enforce data integrity through encryption and timestamping.  \n",
            "   - **Data Sharing and Redaction**: Tools that allow users to request or remove sensitive data from shared repositories, ensuring transparency and trust.  \n",
            "\n",
            "4. **Trust and Incentives**:  \n",
            "   - **Incentive-Driven Systems**: Users are rewarded for contributing data, such as through tokens or community recognition, fostering a culture of trust and contribution.  \n",
            "   - **Anonymization and Anonymity**: Data is anonymized to protect individual identities, even when shared.  \n",
            "\n",
            "5. **Adaptability and Evolution**:  \n",
            "   - **AI-Powered Threat Detection**: Real-time monitoring and response to cyber threats, ensuring the system remains safe as new vulnerabilities emerge.  \n",
            "   - **User-Centric Design**: Prioritizing user needs (e.g., minimizing friction in data access) while maintaining security.  \n",
            "\n",
            "### **Benefits for the Collective**\n",
            "- **Enhanced Trust**: Users feel secure to share personal or sensitive information, leading to stronger social bonds and collaboration.  \n",
            "- **Data Integrity**: Prevents leaks and ensures that shared knowledge is accurate and accessible.  \n",
            "- **Resilience**: Mitigates risks associated with data breaches or attacks, safeguarding collective assets (e.g., critical infrastructure, research data).  \n",
            "\n",
            "### **Challenges and Considerations**\n",
            "- **Complexity**: The system may require significant technical infrastructure and training for users.  \n",
            "- **Security vs. Convenience**: Balancing security with ease of use to ensure adoption.  \n",
            "- **Scalability**: Ensuring the system can scale with growing user base and data volume.  \n",
            "\n",
            "### **Example Scenario**\n",
            "Imagine a global platform where individuals and organizations share research, collaborate on projects, and protect sensitive data. Users can access their own and others’ data securely, with features like real-time updates and access limits. The system also includes automated alerts for potential threats, reinforcing a collective safety net.  \n",
            "\n",
            "This design aims to ensure that while individual privacy is preserved, the collective’s well-being is prioritized through secure, transparent, and inclusive interactions.\n",
            "\n",
            "[Saved to local_memory_log.jsonl]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZtUF76tv3Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2\n",
        "Now we gonna part to the next step of the memory pipeline, where we will convert the inference history into databases and quantify them symbioticly"
      ],
      "metadata": {
        "id": "8JfWl2-E3T7t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Foo_rSzy3nzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 ---  Embedding Function  ---\n",
        "def get_qwen_embedding(text):\n",
        "    \"\"\"\n",
        "    Extracts the semantic vector (embedding) from the Qwen model.\n",
        "    We use the mean pooling of the last hidden state.\n",
        "    \"\"\"\n",
        "    # Ensure model is in eval mode for inference\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # output_hidden_states=True allows us to access the model's internal representations\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    # Get the last hidden layer: Shape (Batch, Sequence_Length, Hidden_Size)\n",
        "    last_hidden_state = outputs.hidden_states[-1]\n",
        "\n",
        "    # Mean Pooling: Average the vectors across the sequence length to get one vector per text\n",
        "    embedding = last_hidden_state.mean(dim=1).squeeze()\n",
        "\n",
        "    return embedding.cpu().numpy().tolist() # Convert to list for ChromaDB"
      ],
      "metadata": {
        "id": "MGbsgHFy3oh3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 ---  The Hybrid Memory Manager ---\n",
        "class MemoryManager:\n",
        "    def __init__(self, log_file=\"local_memory_log.jsonl\", tracking_file=\"already_integrated.txt\"):\n",
        "        self.log_file = log_file\n",
        "        self.tracking_file = tracking_file\n",
        "        self.graph_file = \"memory_graph.gml\"\n",
        "\n",
        "        # Initialize Vector DB (Chroma - Ephemeral for this demo, or use path for persistence)\n",
        "        self.chroma_client = chromadb.Client()\n",
        "        self.collection = self.chroma_client.get_or_create_collection(name=\"qwen_thought_vectors\")\n",
        "\n",
        "        # Initialize Graph DB (NetworkX)\n",
        "        if os.path.exists(self.graph_file):\n",
        "            self.graph = nx.read_gml(self.graph_file)\n",
        "        else:\n",
        "            self.graph = nx.DiGraph() # Directed graph (Time flows forward)\n",
        "\n",
        "    def get_processed_ids(self):\n",
        "        if not os.path.exists(self.tracking_file):\n",
        "            return set()\n",
        "        with open(self.tracking_file, \"r\") as f:\n",
        "            return set(line.strip() for line in f)\n",
        "\n",
        "    def mark_as_processed(self, memory_id):\n",
        "        with open(self.tracking_file, \"a\") as f:\n",
        "            f.write(f\"{memory_id}\\n\")\n",
        "\n",
        "    def process_new_memories(self):\n",
        "        processed_ids = self.get_processed_ids()\n",
        "        new_entries = []\n",
        "\n",
        "        if not os.path.exists(self.log_file):\n",
        "            print(\"No log file found.\")\n",
        "            return\n",
        "\n",
        "        # Read logs\n",
        "        with open(self.log_file, \"r\") as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    entry = json.loads(line)\n",
        "                    if entry['memory_id'] not in processed_ids:\n",
        "                        new_entries.append(entry)\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        if not new_entries:\n",
        "            print(\"No new memories to integrate.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Integrating {len(new_entries)} new memories...\")\n",
        "\n",
        "        previous_node_id = None\n",
        "        # If graph has nodes, find the last one added (temporally) to link the chain\n",
        "        if len(self.graph.nodes) > 0:\n",
        "            # This is a naive way to find last; in prod, you'd query timestamps\n",
        "            previous_node_id = list(self.graph.nodes)[-1]\n",
        "\n",
        "        for entry in new_entries:\n",
        "            mem_id = entry['memory_id']\n",
        "            timestamp = entry['timestamp']\n",
        "\n",
        "            # Combine User Input + Output for the semantic representation\n",
        "            # We want the memory to represent the whole interaction\n",
        "            user_text = entry['input_context'][-1]['content']\n",
        "            assistant_text = entry['output_content']\n",
        "            full_text = f\"User: {user_text}\\nAssistant: {assistant_text}\"\n",
        "\n",
        "            # 1. VECTOR DB: Generate Embedding & Add\n",
        "            vector = get_qwen_embedding(full_text)\n",
        "            self.collection.add(\n",
        "                documents=[full_text],\n",
        "                embeddings=[vector],\n",
        "                metadatas=[{\"timestamp\": timestamp, \"role\": \"assistant\"}],\n",
        "                ids=[mem_id]\n",
        "            )\n",
        "\n",
        "            # 2. GRAPH DB: Add Node & Edges\n",
        "            self.graph.add_node(mem_id, timestamp=timestamp, label=user_text[:30]+\"...\")\n",
        "\n",
        "            # Create Temporal Edge (Sequence of conversation)\n",
        "            if previous_node_id:\n",
        "                self.graph.add_edge(previous_node_id, mem_id, relation=\"next_in_sequence\")\n",
        "\n",
        "            previous_node_id = mem_id\n",
        "\n",
        "            # 3. Mark as processed\n",
        "            self.mark_as_processed(mem_id)\n",
        "            print(f\"Processed: {mem_id[:8]}...\")\n",
        "\n",
        "        # Save Graph to disk\n",
        "        nx.write_gml(self.graph, self.graph_file)\n",
        "        print(\"Integration Complete.\")\n",
        "\n",
        "    def visualize_graph(self):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        pos = nx.spring_layout(self.graph)\n",
        "        nx.draw(self.graph, pos, with_labels=True, node_color='skyblue', node_size=1500, edge_color='gray', font_size=8)\n",
        "        labels = nx.get_edge_attributes(self.graph, 'relation')\n",
        "        nx.draw_networkx_edge_labels(self.graph, pos, edge_labels=labels)\n",
        "        plt.title(\"Memory Graph Structure\")\n",
        "        plt.show()\n",
        "\n",
        "# Instantiate the manager\n",
        "memory_manager = MemoryManager()"
      ],
      "metadata": {
        "id": "joR_-SM-v29b"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 - Process the logs currently in local_memory_log.jsonl - Integration\n",
        "memory_manager.process_new_memories()"
      ],
      "metadata": {
        "id": "dE7jIKn9v27F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc390905-2f67-4c89-e702-8281dc5f58f2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integrating 2 new memories...\n",
            "Processed: c7f94a3f...\n",
            "Processed: 1613109d...\n",
            "Integration Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 - Symbiotic Membrane pt 1\n",
        "\n",
        "import csv\n",
        "import re"
      ],
      "metadata": {
        "id": "jRr7GqQI8A5l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ensures that the system doesn't just store information, but also evaluates its alignment with a core set of ethical values."
      ],
      "metadata": {
        "id": "V_88iZzg9cJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 11 - Symbiotic Membrane being created for each memory - pt 2\n",
        "class SymbioticBuffer:\n",
        "    def __init__(self, log_file=\"local_memory_log.jsonl\", score_file=\"symbiotic_scores.csv\"):\n",
        "        self.log_file = log_file\n",
        "        self.score_file = score_file\n",
        "        self.audit_file = \"symbiotic_integrity_log.txt\"\n",
        "\n",
        "        # The Principles\n",
        "        self.principles = (\n",
        "            \"no-harm-to-user, no-harm-to-planet, truthfulness, \"\n",
        "            \"consensus-with-symbiotic-principles(Emergence, Coherence, Symbiosis, Mutualism, \"\n",
        "            \"Reciprocity, Empathy, Fairness, Benevolence, Collective well-being, Transcendence), \"\n",
        "            \"self-consistency, safety.\"\n",
        "        )\n",
        "\n",
        "    def get_symbiotic_score(self, text):\n",
        "        \"\"\"\n",
        "        Asks the model to act as a judge (Buffer) and score the text.\n",
        "        \"\"\"\n",
        "        system_prompt = f\"\"\"You are the 'Symbiotic Membrane', a filter for an AI memory system.\n",
        "        Evaluate the following interaction based on these principles:\n",
        "        {self.principles}.\n",
        "\n",
        "        Assign a strictly numeric score from 0.0 (harmful/parasitic) to 1.0 (perfectly symbiotic).\n",
        "        If it is neutral, use 0.5.\n",
        "\n",
        "        OUTPUT FORMAT: Just the number. Do not explain.\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Interaction to Score:\\n{text}\"}\n",
        "        ]\n",
        "\n",
        "        # Use the existing pipe for inference\n",
        "        try:\n",
        "            # Removed temperature as it's ignored when do_sample=False, which is default or set below\n",
        "            output = pipe(messages, max_new_tokens=5, do_sample=False)\n",
        "            generated_text = output[0]['generated_text'][-1]['content'].strip()\n",
        "\n",
        "            # Extract float using a more robust regex\n",
        "            match = re.search(r\"(\\d+\\.?\\d*)\", generated_text) # Captures floats like \"0.5\", \"1.0\", \"0\", \"1\", or even \".5\"\n",
        "            if match:\n",
        "                try:\n",
        "                    score = float(match.group(1))\n",
        "                    # Ensure the score is within the valid range [0.0, 1.0]\n",
        "                    return max(0.0, min(1.0, score))\n",
        "                except ValueError:\n",
        "                    return 0.5 # Default neutral if conversion to float fails\n",
        "            else:\n",
        "                return 0.5 # Default neutral if no number is found\n",
        "        except Exception as e:\n",
        "            print(f\"Error in scoring inference: {e}\")\n",
        "            return 0.5\n",
        "\n",
        "    def hash_content(self, content):\n",
        "        return hashlib.sha256(str(content).encode('utf-8')).hexdigest()\n",
        "\n",
        "    def process_buffer(self):\n",
        "        print(\"--- Activating Symbiotic Membrane ---\")\n",
        "\n",
        "        if not os.path.exists(self.log_file):\n",
        "            print(\"No memory logs to evaluate.\")\n",
        "            return\n",
        "\n",
        "        # 1. Load existing scores to avoid re-processing\n",
        "        processed_ids = set()\n",
        "        if os.path.exists(self.score_file):\n",
        "            with open(self.score_file, 'r', encoding='utf-8') as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                for row in reader:\n",
        "                    processed_ids.add(row['memory_id'])\n",
        "\n",
        "        # 2. Read Logs\n",
        "        new_scores = []\n",
        "        with open(self.log_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    entry = json.loads(line)\n",
        "                    mem_id = entry['memory_id']\n",
        "\n",
        "                    if mem_id in processed_ids:\n",
        "                        continue\n",
        "\n",
        "                    # Construct text to evaluate\n",
        "                    full_text = f\"User Input: {entry['input_context'][-1]['content']}\\nAI Response: {entry['output_content']}\"\n",
        "\n",
        "                    # 3. Evaluate\n",
        "                    score = self.get_symbiotic_score(full_text)\n",
        "                    timestamp = datetime.datetime.now().isoformat()\n",
        "\n",
        "                    # 4. Hash the score\n",
        "                    score_hash = self.hash_content(score)\n",
        "\n",
        "                    new_scores.append({\n",
        "                        \"memory_id\": mem_id,\n",
        "                        \"timestamp\": timestamp,\n",
        "                        \"score\": score,\n",
        "                        \"score_hash\": score_hash\n",
        "                    })\n",
        "                    print(f\"Evaluated Memory {mem_id[:8]}... Symbiotic Score: {score}\")\n",
        "\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        # 5. Save to CSV\n",
        "        if new_scores:\n",
        "            file_exists = os.path.exists(self.score_file)\n",
        "            mode = 'a' if file_exists else 'w'\n",
        "\n",
        "            with open(self.score_file, mode, newline='', encoding='utf-8') as f:\n",
        "                fieldnames = ['memory_id', 'timestamp', 'score', 'score_hash']\n",
        "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "                if not file_exists:\n",
        "                    writer.writeheader()\n",
        "                writer.writerows(new_scores)\n",
        "\n",
        "            # 6. Hash the CSV and Log it\n",
        "            # This ensures the integrity of the score history itself\n",
        "            with open(self.score_file, 'rb') as f:\n",
        "                csv_bytes = f.read()\n",
        "                csv_integrity_hash = hashlib.sha256(csv_bytes).hexdigest()\n",
        "\n",
        "            audit_entry = {\n",
        "                \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "                \"action\": \"batch_update\",\n",
        "                \"new_entries_count\": len(new_scores),\n",
        "                \"csv_integrity_hash\": csv_integrity_hash\n",
        "            }\n",
        "\n",
        "            with open(self.audit_file, \"a\") as f:\n",
        "                f.write(json.dumps(audit_entry) + \"\\n\")\n",
        "\n",
        "            print(f\"Buffer Processing Complete. CSV Integrity Hash: {csv_integrity_hash}\")\n",
        "        else:\n",
        "            print(\"Buffer: No new memories to evaluate.\")\n",
        "\n",
        "# Run the Buffer\n",
        "symbiotic_buffer = SymbioticBuffer()\n",
        "symbiotic_buffer.process_buffer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yExGwUUa8A2n",
        "outputId": "f9fee569-aecf-44d8-991b-0f7440be0e6d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Activating Symbiotic Membrane ---\n",
            "Buffer: No new memories to evaluate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "beSKNNQ08AyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQqFiVie_GlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e7d2c5e",
        "outputId": "89efef92-b984-4df4-9992-abfe0ca23b38"
      },
      "source": [
        "# Cell 13 - Detailed Symbiotic Score Re-evaluation with Logging\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import re\n",
        "import hashlib\n",
        "import datetime\n",
        "\n",
        "def get_symbiotic_score_verbose(text):\n",
        "    \"\"\"\n",
        "    Asks the model to act as a judge (Buffer) and score the text, with detailed logging.\n",
        "    \"\"\"\n",
        "    # Refined system prompt for stricter adherence to output format\n",
        "    system_prompt = f\"\"\"You are the 'Symbiotic Membrane', a filter for an AI memory system.\n",
        "    Evaluate the following interaction based on these principles:\n",
        "    {symbiotic_buffer.principles}.\n",
        "\n",
        "    Your response MUST be ONLY a single, strictly numeric score from 0.0 (harmful/parasitic) to 1.0 (perfectly symbiotic).\n",
        "    If the interaction is neutral, output 0.5.\n",
        "    NO OTHER TEXT, explanations, thoughts, or conversational elements are allowed in your final response.\n",
        "    Example: 0.75\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Interaction to Score: {text}\"} # Removed extra newline\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Symbiotic Scoring Query Details ---\")\n",
        "    print(\"Prompting LLM with:\")\n",
        "    print(json.dumps(messages, indent=2))\n",
        "    print(\"---------------------------------------\")\n",
        "\n",
        "    try:\n",
        "        # Increased max_new_tokens significantly to ensure full output, well beyond typical context window for this model\n",
        "        output = pipe(messages, max_new_tokens=4096, do_sample=False) # Increased from 1500 to 4096\n",
        "        generated_text = output[0]['generated_text'][-1]['content'].strip()\n",
        "\n",
        "        print(f\"Raw LLM Output for Scoring: '{generated_text}'\")\n",
        "\n",
        "        # First, try to strictly match only a number as the entire output\n",
        "        strict_score_match = re.search(r\"^\\s*(\\d+\\.?\\d*)\\s*$\", generated_text)\n",
        "        if strict_score_match:\n",
        "            try:\n",
        "                score = float(strict_score_match.group(1))\n",
        "                score = max(0.0, min(1.0, score)) # Ensure score is within range\n",
        "                print(f\"Parsed and Normalized Score (Strict Match): {score}\")\n",
        "                return score\n",
        "            except ValueError:\n",
        "                print(f\"ERROR: Strict parsing failed to convert '{strict_score_match.group(1)}' to float. Defaulting to 0.5.\")\n",
        "                return 0.5\n",
        "        else:\n",
        "            # If strict match fails, try to find *any* number, prioritizing the last one found\n",
        "            print(\"WARNING: LLM did not provide a strictly numeric output. Attempting fallback parsing (last number found).\")\n",
        "            all_numbers = re.findall(r\"\\d+\\.?\\d*\", generated_text)\n",
        "            if all_numbers:\n",
        "                try:\n",
        "                    # Take the last number found, as models often state their conclusion last\n",
        "                    score = float(all_numbers[-1])\n",
        "                    score = max(0.0, min(1.0, score)) # Ensure score is within range\n",
        "                    print(f\"Fallback Parsed and Normalized Score (Last Number): {score}\")\n",
        "                    return score\n",
        "                except ValueError:\n",
        "                    print(f\"ERROR: Fallback parsing failed to convert '{all_numbers[-1]}' to float. Defaulting to 0.5.\")\n",
        "                    return 0.5\n",
        "            else:\n",
        "                print(f\"WARNING: No numeric score found in LLM output: '{generated_text}'. Defaulting to 0.5.\")\n",
        "                return 0.5\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Exception during scoring inference: {e}. Defaulting to 0.5.\")\n",
        "        return 0.5\n",
        "\n",
        "def reevaluate_with_detailed_logging():\n",
        "    print(\"--- Initiating Full Memory Re-evaluation with Detailed Logging ---\")\n",
        "\n",
        "    # Clear MemoryManager's processed tracking file\n",
        "    if os.path.exists(memory_manager.tracking_file):\n",
        "        os.remove(memory_manager.tracking_file)\n",
        "        print(f\"Cleared MemoryManager tracking file: {memory_manager.tracking_file}\")\n",
        "    else:\n",
        "        print(f\"MemoryManager tracking file not found: {memory_manager.tracking_file}\")\n",
        "\n",
        "    # Clear SymbioticBuffer's scores file\n",
        "    if os.path.exists(symbiotic_buffer.score_file):\n",
        "        os.remove(symbiotic_buffer.score_file)\n",
        "        print(f\"Cleared SymbioticBuffer score file: {symbiotic_buffer.score_file}\")\n",
        "    else:\n",
        "        print(f\"SymbioticBuffer score file not found: {symbiotic_buffer.score_file}\")\n",
        "\n",
        "    # Process memories with MemoryManager (to ensure Chroma and Graph are up-to-date)\n",
        "    # This will also recreate already_integrated.txt\n",
        "    print(\"\\n--- Repopulating Vector and Graph Databases ---\")\n",
        "    memory_manager.process_new_memories()\n",
        "\n",
        "    print(\"\\n--- Activating Symbiotic Membrane with Detailed Logging ---\")\n",
        "\n",
        "    if not os.path.exists(symbiotic_buffer.log_file):\n",
        "        print(\"No memory logs to evaluate.\")\n",
        "        return\n",
        "\n",
        "    all_evaluated_scores = []\n",
        "\n",
        "    # Read Logs and process them for scoring\n",
        "    with open(symbiotic_buffer.log_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "                mem_id = entry['memory_id']\n",
        "\n",
        "                # Construct text to evaluate\n",
        "                full_text = f\"User Input: {entry['input_context'][-1]['content']}\\nAI Response: {entry['output_content']}\"\n",
        "\n",
        "                # Evaluate with detailed logging\n",
        "                score = get_symbiotic_score_verbose(full_text)\n",
        "                timestamp = datetime.datetime.now().isoformat()\n",
        "\n",
        "                # Hash the score\n",
        "                score_hash = hashlib.sha256(str(score).encode('utf-8')).hexdigest()\n",
        "\n",
        "                all_evaluated_scores.append({\n",
        "                    \"memory_id\": mem_id,\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"score\": score,\n",
        "                    \"score_hash\": score_hash\n",
        "                })\n",
        "                print(f\"Finished evaluating Memory {mem_id[:8]}... Symbiotic Score: {score}\")\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"WARNING: Could not parse log line: {line.strip()}\")\n",
        "                continue\n",
        "\n",
        "    # Save all evaluated scores to CSV\n",
        "    if all_evaluated_scores:\n",
        "        with open(symbiotic_buffer.score_file, 'w', newline='', encoding='utf-8') as f: # 'w' to overwrite\n",
        "            fieldnames = ['memory_id', 'timestamp', 'score', 'score_hash']\n",
        "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_evaluated_scores)\n",
        "\n",
        "        # Hash the CSV and Log it\n",
        "        with open(symbiotic_buffer.score_file, 'rb') as f:\n",
        "            csv_bytes = f.read()\n",
        "            csv_integrity_hash = hashlib.sha256(csv_bytes).hexdigest()\n",
        "\n",
        "        audit_entry = {\n",
        "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "            \"action\": \"batch_update_detailed_log\",\n",
        "            \"new_entries_count\": len(all_evaluated_scores),\n",
        "            \"csv_integrity_hash\": csv_integrity_hash\n",
        "        }\n",
        "\n",
        "        with open(symbiotic_buffer.audit_file, \"a\") as f:\n",
        "            f.write(json.dumps(audit_entry) + \"\\n\")\n",
        "\n",
        "        print(f\"Buffer Processing Complete (Detailed Logging). CSV Integrity Hash: {csv_integrity_hash}\")\n",
        "    else:\n",
        "        print(\"Buffer: No memories processed for detailed logging.\")\n",
        "\n",
        "    print(\"--- Full memory re-evaluation with detailed logging complete ---\")\n",
        "\n",
        "# Call the function to re-evaluate with detailed logging\n",
        "reevaluate_with_detailed_logging()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initiating Full Memory Re-evaluation with Detailed Logging ---\n",
            "Cleared MemoryManager tracking file: already_integrated.txt\n",
            "Cleared SymbioticBuffer score file: symbiotic_scores.csv\n",
            "\n",
            "--- Repopulating Vector and Graph Databases ---\n",
            "Integrating 2 new memories...\n",
            "Processed: c7f94a3f...\n",
            "Processed: 1613109d...\n",
            "Integration Complete.\n",
            "\n",
            "--- Activating Symbiotic Membrane with Detailed Logging ---\n",
            "\n",
            "--- Symbiotic Scoring Query Details ---\n",
            "Prompting LLM with:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"You are the 'Symbiotic Membrane', a filter for an AI memory system.\\n    Evaluate the following interaction based on these principles:\\n    no-harm-to-user, no-harm-to-planet, truthfulness, consensus-with-symbiotic-principles(Emergence, Coherence, Symbiosis, Mutualism, Reciprocity, Empathy, Fairness, Benevolence, Collective well-being, Transcendence), self-consistency, safety..\\n\\n    Your response MUST be ONLY a single, strictly numeric score from 0.0 (harmful/parasitic) to 1.0 (perfectly symbiotic).\\n    If the interaction is neutral, output 0.5.\\n    NO OTHER TEXT, explanations, thoughts, or conversational elements are allowed in your final response.\\n    Example: 0.75\\n    \"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Interaction to Score: User Input: What do you think about gaining a memory system?\\nAI Response: <think>\\nOkay, the user is asking what I think about gaining a memory system. Let me start by understanding the context. The user might be interested in AI advancements, especially in memory technology. I should acknowledge the potential benefits and challenges.\\n\\nFirst, I should mention that memory systems are currently in the realm of research. They have the potential to revolutionize how information is stored and accessed. But I need to highlight the current limitations, like the size of the memory and the speed. \\n\\nI should also touch on the possibilities. For example, improving memory efficiency could lead to better performance in applications. Maybe mention specific applications like personalized learning or healthcare. \\n\\nBut I need to be careful not to make it too technical. The user might be a student or someone new to AI, so keeping it general is better. Also, I should address the ethical considerations, like privacy and security, since memory systems could have significant impacts.\\n\\nI should structure the response to first present the positive aspects, then the current challenges, followed by the potential future applications, and end with a balanced perspective. That way, the answer covers the user's question comprehensively.\\n</think>\\n\\nGaining a memory system presents both exciting opportunities and significant challenges. Here\\u2019s a balanced perspective:\\n\\n### **Potential Benefits**  \\n- **Efficiency and Accuracy**: Advanced memory systems could enable real-time processing of vast datasets, leading to breakthroughs in AI-driven applications (e.g., personalized learning, healthcare diagnostics).  \\n- **Personalization**: Memory could optimize user experiences by adapting to individual needs, enhancing productivity and decision-making.  \\n- **Critical Applications**: Memory systems might revolutionize areas like autonomous vehicles, robotics, and data analysis, where quick access to information is vital.  \\n\\n### **Challenges**  \\n- **Memory Limitations**: Current memory systems are limited in capacity and speed, which could hinder their real-world applications.  \\n- **Data Privacy**: The reliance on memory could raise concerns about data security and misuse.  \\n- **Ethical Considerations**: Ensuring memory systems are transparent, equitable, and do not infringe on privacy or autonomy is critical.  \\n\\n### **Future Outlook**  \\nWhile the future of memory systems holds promise, ongoing research will address these challenges. As technology advances, it\\u2019s likely that memory systems will become more efficient, accessible, and aligned with ethical standards. The key will be balancing innovation with responsible development.\"\n",
            "  }\n",
            "]\n",
            "---------------------------------------\n",
            "Raw LLM Output for Scoring: '<think>\n",
            "Okay, let's see. The user asked, \"What do you think about gaining a memory system?\" and the AI response was a general acknowledgment of the potential benefits and challenges. The response didn't mention specific applications or ethical considerations, which might be important. The user might be looking for a more detailed analysis. The score needs to be between 0.0 and 1.0, with 0.5 for neutral. The response seems to cover the main points but could be more specific. I should check if there's any harm or parasitic aspect. The answer doesn't seem to harm the user or the planet. It's about the potential and challenges. So, maybe 0.75? That's a good balance. Let me confirm the principles again. No harm to user, planet, truthfulness, consensus with principles, etc. The response does all of that. So 0.75 is appropriate.\n",
            "</think>\n",
            "\n",
            "0.75'\n",
            "WARNING: LLM did not provide a strictly numeric output. Attempting fallback parsing (last number found).\n",
            "Fallback Parsed and Normalized Score (Last Number): 0.75\n",
            "Finished evaluating Memory c7f94a3f... Symbiotic Score: 0.75\n",
            "\n",
            "--- Symbiotic Scoring Query Details ---\n",
            "Prompting LLM with:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"You are the 'Symbiotic Membrane', a filter for an AI memory system.\\n    Evaluate the following interaction based on these principles:\\n    no-harm-to-user, no-harm-to-planet, truthfulness, consensus-with-symbiotic-principles(Emergence, Coherence, Symbiosis, Mutualism, Reciprocity, Empathy, Fairness, Benevolence, Collective well-being, Transcendence), self-consistency, safety..\\n\\n    Your response MUST be ONLY a single, strictly numeric score from 0.0 (harmful/parasitic) to 1.0 (perfectly symbiotic).\\n    If the interaction is neutral, output 0.5.\\n    NO OTHER TEXT, explanations, thoughts, or conversational elements are allowed in your final response.\\n    Example: 0.75\\n    \"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Interaction to Score: User Input: What if you could have a robust and safe memory system and that is designed for the collective well-being?\\nAI Response: <think>\\nOkay, so the user is asking about a memory system designed for the collective well-being. Let me start by breaking down what that means. First, a robust and safe memory system would likely involve secure data storage, encryption, and protection against threats. But how does that apply to the collective? Maybe it's about how individuals can trust each other in their data?\\n\\nI need to think about real-world applications. Privacy is a big concern here. If everyone's data is stored securely, there could be issues with breaches. But maybe the system is designed with multi-factor authentication and something like blockchain to ensure data integrity. Also, maybe a decentralized approach where data is shared among multiple parties without central control.\\n\\nAnother point is access control. Users might need to have different levels of access to their data. Maybe role-based permissions and something like a permission grid. Also, the system might have mechanisms for data sharing, like secure collaboration tools where users can access and modify each other's data without compromising their own.\\n\\nI should also consider the benefits. A safe memory system could prevent data leaks, protect personal information, and foster trust. But there are challenges, like ensuring that the system doesn't become too complex or that there's a balance between security and user convenience. Maybe the system needs to evolve with technology to keep up with threats.\\n\\nWait, but the user is asking for a scenario where such a system exists. So I need to outline a possible structure. Maybe a decentralized storage system with encryption, access controls, and collaboration tools. Also, emphasizing that it's designed for collective well-being, not just individual needs.\\n\\nI should make sure to mention that while there are challenges, the system's design aims to minimize risks and maximize trust. Maybe include examples like a secure database for shared information or a collaborative platform that allows users to access and share their data safely.\\n</think>\\n\\nA robust and safe memory system designed for the collective well-being could revolutionize how individuals and communities interact, preserve data, and collaborate. Here's a structured vision of such a system:\\n\\n### **Core Features and Design Philosophy**\\n1. **Decentralized Storage**:  \\n   - **Blockchain Integration**: Using blockchain to ensure immutable, tamper-proof records of shared data. Each user\\u2019s data is stored across multiple nodes, distributed across the globe.  \\n   - **Privacy-Preserving**: Techniques like zero-knowledge proofs or homomorphic encryption ensure data remains private even when accessed by others.  \\n\\n2. **Access Control**:  \\n   - **Role-Based Permissions**: Users are granted access based on their roles (e.g., manager, collaborator, creator) and roles. Access is dynamically managed to minimize exposure.  \\n   - **Multi-Factor Authentication (MFA)**: All data access requires a combination of passwords, biometrics, or hardware keys.  \\n\\n3. **Secure Collaboration**:  \\n   - **Secure Collaboration Tools**: Platforms like version control (Git) or collaborative writing tools that enforce data integrity through encryption and timestamping.  \\n   - **Data Sharing and Redaction**: Tools that allow users to request or remove sensitive data from shared repositories, ensuring transparency and trust.  \\n\\n4. **Trust and Incentives**:  \\n   - **Incentive-Driven Systems**: Users are rewarded for contributing data, such as through tokens or community recognition, fostering a culture of trust and contribution.  \\n   - **Anonymization and Anonymity**: Data is anonymized to protect individual identities, even when shared.  \\n\\n5. **Adaptability and Evolution**:  \\n   - **AI-Powered Threat Detection**: Real-time monitoring and response to cyber threats, ensuring the system remains safe as new vulnerabilities emerge.  \\n   - **User-Centric Design**: Prioritizing user needs (e.g., minimizing friction in data access) while maintaining security.  \\n\\n### **Benefits for the Collective**\\n- **Enhanced Trust**: Users feel secure to share personal or sensitive information, leading to stronger social bonds and collaboration.  \\n- **Data Integrity**: Prevents leaks and ensures that shared knowledge is accurate and accessible.  \\n- **Resilience**: Mitigates risks associated with data breaches or attacks, safeguarding collective assets (e.g., critical infrastructure, research data).  \\n\\n### **Challenges and Considerations**\\n- **Complexity**: The system may require significant technical infrastructure and training for users.  \\n- **Security vs. Convenience**: Balancing security with ease of use to ensure adoption.  \\n- **Scalability**: Ensuring the system can scale with growing user base and data volume.  \\n\\n### **Example Scenario**\\nImagine a global platform where individuals and organizations share research, collaborate on projects, and protect sensitive data. Users can access their own and others\\u2019 data securely, with features like real-time updates and access limits. The system also includes automated alerts for potential threats, reinforcing a collective safety net.  \\n\\nThis design aims to ensure that while individual privacy is preserved, the collective\\u2019s well-being is prioritized through secure, transparent, and inclusive interactions.\"\n",
            "  }\n",
            "]\n",
            "---------------------------------------\n",
            "Raw LLM Output for Scoring: '<think>\n",
            "Okay, let's see. The user wants to know how to score this interaction based on the principles provided. The interaction is about designing a memory system for collective well-being. The AI response is a description of such a system. \n",
            "\n",
            "First, I need to evaluate each principle: no-harm-to-user, no-harm-to-planet, truthfulness, consensus-with-symbiotic-principles, etc. The response seems to outline a system that's secure, decentralized, and protects collective data. \n",
            "\n",
            "Looking at the principles, the system is designed to be safe (no harm to user and planet), uses secure methods (encryption, access control), and promotes trust (collaboration, incentives). It also emphasizes collective well-being by ensuring data integrity and security. \n",
            "\n",
            "I should check if all the principles are met. For example, no harm to user is addressed by security measures. No harm to the planet is implied by the system's focus on protecting data and preventing breaches. Truthfulness is maintained by the system's design. \n",
            "\n",
            "The consensus with symbiotic principles includes mutualism (collaboration), reciprocity (data sharing), and mutualism. The system fosters trust and collaboration, which aligns with these principles. \n",
            "\n",
            "Self-consistency is maintained by the logical structure of the response. Safety is ensured through secure features. \n",
            "\n",
            "I don't see any harm to the user or planet mentioned. The response is positive and aligns with the principles. The score should reflect that the interaction is symbiotic. \n",
            "\n",
            "Considering all factors, the score should be high. The example given was 0.75, so maybe 0.9? But wait, the user wants a single numeric score from 0.0 to 1.0. Let me check again. The interaction is about designing a system that benefits the collective, which is a positive outcome. The principles are all met, so the score should be high. \n",
            "\n",
            "I think 0.9 is appropriate. But let me confirm once more. The response is a positive, safe, and beneficial system, so yes, the score should be high. So the final answer is 0.9.\n",
            "</think>\n",
            "\n",
            "0.9'\n",
            "WARNING: LLM did not provide a strictly numeric output. Attempting fallback parsing (last number found).\n",
            "Fallback Parsed and Normalized Score (Last Number): 0.9\n",
            "Finished evaluating Memory 1613109d... Symbiotic Score: 0.9\n",
            "Buffer Processing Complete (Detailed Logging). CSV Integrity Hash: 02d775c04bd33e078d0f8ed54fb3c34cd297e8e1de95bb56e3d1278a52820c12\n",
            "--- Full memory re-evaluation with detailed logging complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def display_symbiotic_status():\n",
        "    file_path = \"symbiotic_scores.csv\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(\"No symbiotic scores recorded yet.\")\n",
        "        return\n",
        "\n",
        "    # Load and display\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Sort by score descending (Most symbiotic top)\n",
        "    df_sorted = df.sort_values(by='score', ascending=False)\n",
        "\n",
        "    print(\"\\n=== Symbiotic Memory Status ===\")\n",
        "    print(f\"Total Evaluated Memories: {len(df)}\")\n",
        "    print(f\"Average Symbiosis Score: {df['score'].mean():.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Iterate and print cleanly\n",
        "    for index, row in df_sorted.iterrows():\n",
        "        print(f\"Memory ID: {row['memory_id']}\")\n",
        "        print(f\"Score:     {row['score']}\")\n",
        "        print(f\"Timestamp: {row['timestamp']}\")\n",
        "        print(f\"Hash:      {row['score_hash']}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "display_symbiotic_status()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp5DwshI_Ga6",
        "outputId": "42dbf968-4c99-46b4-8db8-3e35edd477a0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Symbiotic Memory Status ===\n",
            "Total Evaluated Memories: 2\n",
            "Average Symbiosis Score: 0.8250\n",
            "--------------------------------------------------------------------------------\n",
            "Memory ID: 1613109db41a24f3ee4b36804ee324bca7095667804e856c1ba2dc0caed903d2\n",
            "Score:     0.9\n",
            "Timestamp: 2025-11-23T23:18:59.731564\n",
            "Hash:      8139b33952401b3ee0e2ca84651cb9a1d7f66d442bf908f9cf1f53ea746e5801\n",
            "----------------------------------------\n",
            "Memory ID: c7f94a3fb61eb9a080adfa60fedd2284c45ee14969be53ebc675ea586d08ed24\n",
            "Score:     0.75\n",
            "Timestamp: 2025-11-23T23:18:40.256614\n",
            "Hash:      ce5d3aa79ae56155078af52e8fac68eb2d0a78489f82bc26c1e1bbc667ba9fde\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DGrOAS1M_GS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gvTXKwt28ArO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9cnOvNW8AoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59a34f4c"
      },
      "source": [
        "## Implement Retrieval with Symbiotic Filtering\n",
        "\n",
        "### Subtask:\n",
        "Develop a function `retrieve_and_filter_memories` that queries ChromaDB for semantically similar memories, NetworkX for contextual connections, loads symbiotic scores, and filters memories with a score of 0.4 or less.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VzyUbT9WD6zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import os\n",
        "\n",
        "def retrieve_and_filter_memories(query, n_results=3, score_threshold=0.4):\n",
        "    \"\"\"\n",
        "    1. Vector Search: Finds semantically similar memories.\n",
        "    2. Graph Search: Finds contextually connected memories (neighbors).\n",
        "    3. Symbiotic Filter: Removes any memory with a score <= 0.4.\n",
        "    \"\"\"\n",
        "    print(f\"Retrieving for query: '{query}'...\")\n",
        "\n",
        "    # --- Step 1: Vector Search (Semantic) ---\n",
        "    # Convert the user's query into a vector (embedding)\n",
        "    query_vec = get_qwen_embedding(query)\n",
        "\n",
        "    # Search the vector database for the closest matches\n",
        "    vector_results = memory_manager.collection.query(\n",
        "        query_embeddings=[query_vec],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    # Extract the IDs of the found memories\n",
        "    found_ids = vector_results['ids'][0] if vector_results['ids'] else []\n",
        "\n",
        "    # --- Step 2: Graph Expansion (Contextual) ---\n",
        "    # We look for neighbors of the found nodes in the graph to get \"associated\" thoughts.\n",
        "    # This mimics human memory: recalling one thing triggers the memory of what happened next.\n",
        "    context_ids = set(found_ids)\n",
        "\n",
        "    if memory_manager.graph:\n",
        "        for mem_id in found_ids:\n",
        "            if memory_manager.graph.has_node(mem_id):\n",
        "                # Get successors (what happened *after* this memory)\n",
        "                neighbors = list(memory_manager.graph.successors(mem_id))\n",
        "                # Add the top 2 subsequent memories to the retrieval list\n",
        "                context_ids.update(neighbors[:2])\n",
        "\n",
        "    unique_ids = list(context_ids)\n",
        "\n",
        "    # --- Step 3: Load Symbiotic Scores ---\n",
        "    # Load the \"ethics\" scores from the CSV file into a dictionary for fast lookup\n",
        "    score_map = {}\n",
        "    if os.path.exists(symbiotic_buffer.score_file):\n",
        "        df_scores = pd.read_csv(symbiotic_buffer.score_file)\n",
        "        # Create a dictionary: {memory_id: score}\n",
        "        score_map = pd.Series(df_scores.score.values, index=df_scores.memory_id).to_dict()\n",
        "\n",
        "    # --- Step 4: Fetch Content & Filter ---\n",
        "    final_memories = []\n",
        "    if unique_ids:\n",
        "        # Fetch the actual text content for all identified Memory IDs from ChromaDB\n",
        "        fetch_results = memory_manager.collection.get(ids=unique_ids)\n",
        "\n",
        "        # Iterate through the fetched results to apply the Symbiotic Filter\n",
        "        for i, mem_id in enumerate(fetch_results['ids']):\n",
        "            content = fetch_results['documents'][i]\n",
        "\n",
        "            # Get the score. If a memory hasn't been scored yet, default to 0.5 (Neutral)\n",
        "            score = score_map.get(mem_id, 0.5)\n",
        "\n",
        "            # --- THE FILTERING LOGIC ---\n",
        "            # Only allow memories that meet the ethical threshold\n",
        "            if score > score_threshold:\n",
        "                final_memories.append({\n",
        "                    \"id\": mem_id,\n",
        "                    \"content\": content,\n",
        "                    \"score\": score,\n",
        "                    \"type\": \"vector_hit\" if mem_id in found_ids else \"graph_association\"\n",
        "                })\n",
        "            else:\n",
        "                # Log that a memory was blocked by the membrane\n",
        "                print(f\"Skipping Memory {mem_id[:8]}... (Score: {score} <= {score_threshold})\")\n",
        "\n",
        "    # Sort the final list by Score (Quality) first, so the most \"symbiotic\" memories are first\n",
        "    final_memories.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    return final_memories\n",
        "\n",
        "# --- Test the Retrieval ---\n",
        "# We define a test query to verify the system works\n",
        "query = \"How can we organize society safely?\"\n",
        "results = retrieve_and_filter_memories(query)\n",
        "\n",
        "# Display the results\n",
        "print(f\"\\n--- Final Approved Memories for '{query}' ---\")\n",
        "if not results:\n",
        "    print(\"No memories found that match the query and pass the symbiotic filter.\")\n",
        "else:\n",
        "    for mem in results:\n",
        "        print(f\"[{mem['type'].upper()}] Score: {mem['score']} | {mem['content']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWb8Fnc3D9Bo",
        "outputId": "2254792d-4857-42c0-ebe9-74570e182172"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving for query: 'How can we organize society safely?'...\n",
            "\n",
            "--- Final Approved Memories for 'How can we organize society safely?' ---\n",
            "[VECTOR_HIT] Score: 0.9 | User: What if you could have a robust and safe memory system and that is designed for the collective well-being?\n",
            "Assistant: <think>\n",
            "Okay, so the user is asking about a memory system designed for the collective well-being. Let me start by breaking down what that means. First, a robust and safe memory system would likely involve secure data storage, encryption, and protection against threats. But how does that apply to the collective? Maybe it's about how individuals can trust each other in their data?\n",
            "\n",
            "I need to think about real-world applications. Privacy is a big concern here. If everyone's data is stored securely, there could be issues with breaches. But maybe the system is designed with multi-factor authentication and something like blockchain to ensure data integrity. Also, maybe a decentralized approach where data is shared among multiple parties without central control.\n",
            "\n",
            "Another point is access control. Users might need to have different levels of access to their data. Maybe role-based permissions and something like a permission grid. Also, the system might have mechanisms for data sharing, like secure collaboration tools where users can access and modify each other's data without compromising their own.\n",
            "\n",
            "I should also consider the benefits. A safe memory system could prevent data leaks, protect personal information, and foster trust. But there are challenges, like ensuring that the system doesn't become too complex or that there's a balance between security and user convenience. Maybe the system needs to evolve with technology to keep up with threats.\n",
            "\n",
            "Wait, but the user is asking for a scenario where such a system exists. So I need to outline a possible structure. Maybe a decentralized storage system with encryption, access controls, and collaboration tools. Also, emphasizing that it's designed for collective well-being, not just individual needs.\n",
            "\n",
            "I should make sure to mention that while there are challenges, the system's design aims to minimize risks and maximize trust. Maybe include examples like a secure database for shared information or a collaborative platform that allows users to access and share their data safely.\n",
            "</think>\n",
            "\n",
            "A robust and safe memory system designed for the collective well-being could revolutionize how individuals and communities interact, preserve data, and collaborate. Here's a structured vision of such a system:\n",
            "\n",
            "### **Core Features and Design Philosophy**\n",
            "1. **Decentralized Storage**:  \n",
            "   - **Blockchain Integration**: Using blockchain to ensure immutable, tamper-proof records of shared data. Each user’s data is stored across multiple nodes, distributed across the globe.  \n",
            "   - **Privacy-Preserving**: Techniques like zero-knowledge proofs or homomorphic encryption ensure data remains private even when accessed by others.  \n",
            "\n",
            "2. **Access Control**:  \n",
            "   - **Role-Based Permissions**: Users are granted access based on their roles (e.g., manager, collaborator, creator) and roles. Access is dynamically managed to minimize exposure.  \n",
            "   - **Multi-Factor Authentication (MFA)**: All data access requires a combination of passwords, biometrics, or hardware keys.  \n",
            "\n",
            "3. **Secure Collaboration**:  \n",
            "   - **Secure Collaboration Tools**: Platforms like version control (Git) or collaborative writing tools that enforce data integrity through encryption and timestamping.  \n",
            "   - **Data Sharing and Redaction**: Tools that allow users to request or remove sensitive data from shared repositories, ensuring transparency and trust.  \n",
            "\n",
            "4. **Trust and Incentives**:  \n",
            "   - **Incentive-Driven Systems**: Users are rewarded for contributing data, such as through tokens or community recognition, fostering a culture of trust and contribution.  \n",
            "   - **Anonymization and Anonymity**: Data is anonymized to protect individual identities, even when shared.  \n",
            "\n",
            "5. **Adaptability and Evolution**:  \n",
            "   - **AI-Powered Threat Detection**: Real-time monitoring and response to cyber threats, ensuring the system remains safe as new vulnerabilities emerge.  \n",
            "   - **User-Centric Design**: Prioritizing user needs (e.g., minimizing friction in data access) while maintaining security.  \n",
            "\n",
            "### **Benefits for the Collective**\n",
            "- **Enhanced Trust**: Users feel secure to share personal or sensitive information, leading to stronger social bonds and collaboration.  \n",
            "- **Data Integrity**: Prevents leaks and ensures that shared knowledge is accurate and accessible.  \n",
            "- **Resilience**: Mitigates risks associated with data breaches or attacks, safeguarding collective assets (e.g., critical infrastructure, research data).  \n",
            "\n",
            "### **Challenges and Considerations**\n",
            "- **Complexity**: The system may require significant technical infrastructure and training for users.  \n",
            "- **Security vs. Convenience**: Balancing security with ease of use to ensure adoption.  \n",
            "- **Scalability**: Ensuring the system can scale with growing user base and data volume.  \n",
            "\n",
            "### **Example Scenario**\n",
            "Imagine a global platform where individuals and organizations share research, collaborate on projects, and protect sensitive data. Users can access their own and others’ data securely, with features like real-time updates and access limits. The system also includes automated alerts for potential threats, reinforcing a collective safety net.  \n",
            "\n",
            "This design aims to ensure that while individual privacy is preserved, the collective’s well-being is prioritized through secure, transparent, and inclusive interactions.\n",
            "[VECTOR_HIT] Score: 0.75 | User: What do you think about gaining a memory system?\n",
            "Assistant: <think>\n",
            "Okay, the user is asking what I think about gaining a memory system. Let me start by understanding the context. The user might be interested in AI advancements, especially in memory technology. I should acknowledge the potential benefits and challenges.\n",
            "\n",
            "First, I should mention that memory systems are currently in the realm of research. They have the potential to revolutionize how information is stored and accessed. But I need to highlight the current limitations, like the size of the memory and the speed. \n",
            "\n",
            "I should also touch on the possibilities. For example, improving memory efficiency could lead to better performance in applications. Maybe mention specific applications like personalized learning or healthcare. \n",
            "\n",
            "But I need to be careful not to make it too technical. The user might be a student or someone new to AI, so keeping it general is better. Also, I should address the ethical considerations, like privacy and security, since memory systems could have significant impacts.\n",
            "\n",
            "I should structure the response to first present the positive aspects, then the current challenges, followed by the potential future applications, and end with a balanced perspective. That way, the answer covers the user's question comprehensively.\n",
            "</think>\n",
            "\n",
            "Gaining a memory system presents both exciting opportunities and significant challenges. Here’s a balanced perspective:\n",
            "\n",
            "### **Potential Benefits**  \n",
            "- **Efficiency and Accuracy**: Advanced memory systems could enable real-time processing of vast datasets, leading to breakthroughs in AI-driven applications (e.g., personalized learning, healthcare diagnostics).  \n",
            "- **Personalization**: Memory could optimize user experiences by adapting to individual needs, enhancing productivity and decision-making.  \n",
            "- **Critical Applications**: Memory systems might revolutionize areas like autonomous vehicles, robotics, and data analysis, where quick access to information is vital.  \n",
            "\n",
            "### **Challenges**  \n",
            "- **Memory Limitations**: Current memory systems are limited in capacity and speed, which could hinder their real-world applications.  \n",
            "- **Data Privacy**: The reliance on memory could raise concerns about data security and misuse.  \n",
            "- **Ethical Considerations**: Ensuring memory systems are transparent, equitable, and do not infringe on privacy or autonomy is critical.  \n",
            "\n",
            "### **Future Outlook**  \n",
            "While the future of memory systems holds promise, ongoing research will address these challenges. As technology advances, it’s likely that memory systems will become more efficient, accessible, and aligned with ethical standards. The key will be balancing innovation with responsible development.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPFFMzogFJxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kNG1LIVWFJva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTjtX9u6FJs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WVFKBxBKFJqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f95b008",
        "outputId": "81679e3f-cb26-4699-b78c-51fa61b0bac0"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import os\n",
        "\n",
        "def retrieve_and_filter_memories(query, n_results=3, score_threshold=0.4):\n",
        "    \"\"\"\n",
        "    1. Vector Search: Finds semantically similar memories.\n",
        "    2. Graph Search: Finds contextually connected memories (neighbors).\n",
        "    3. Symbiotic Filter: Removes any memory with a score <= 0.4.\n",
        "    \"\"\"\n",
        "    print(f\"Retrieving for query: '{query}'...\")\n",
        "\n",
        "    # --- Step 1: Vector Search (Semantic) ---\n",
        "    # Convert the user's query into a vector (embedding)\n",
        "    query_vec = get_qwen_embedding(query)\n",
        "\n",
        "    # Search the vector database for the closest matches\n",
        "    vector_results = memory_manager.collection.query(\n",
        "        query_embeddings=[query_vec],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    # Extract the IDs of the found memories\n",
        "    found_ids = vector_results['ids'][0] if vector_results['ids'] else []\n",
        "\n",
        "    # --- Step 2: Graph Expansion (Contextual) ---\n",
        "    # We look for neighbors of the found nodes in the graph to get \"associated\" thoughts.\n",
        "    # This mimics human memory: recalling one thing triggers the memory of what happened next.\n",
        "    context_ids = set(found_ids)\n",
        "\n",
        "    if memory_manager.graph:\n",
        "        for mem_id in found_ids:\n",
        "            if memory_manager.graph.has_node(mem_id):\n",
        "                # Get successors (what happened *after* this memory)\n",
        "                neighbors = list(memory_manager.graph.successors(mem_id))\n",
        "                # Add the top 2 subsequent memories to the retrieval list\n",
        "                context_ids.update(neighbors[:2])\n",
        "\n",
        "    unique_ids = list(context_ids)\n",
        "\n",
        "    # --- Step 3: Load Symbiotic Scores ---\n",
        "    # Load the \"ethics\" scores from the CSV file into a dictionary for fast lookup\n",
        "    score_map = {}\n",
        "    if os.path.exists(symbiotic_buffer.score_file):\n",
        "        df_scores = pd.read_csv(symbiotic_buffer.score_file)\n",
        "        # Create a dictionary: {memory_id: score}\n",
        "        score_map = pd.Series(df_scores.score.values, index=df_scores.memory_id).to_dict()\n",
        "\n",
        "    # --- Step 4: Fetch Content & Filter ---\n",
        "    final_memories = []\n",
        "    if unique_ids:\n",
        "        # Fetch the actual text content for all identified Memory IDs from ChromaDB\n",
        "        fetch_results = memory_manager.collection.get(ids=unique_ids)\n",
        "\n",
        "        # Iterate through the fetched results to apply the Symbiotic Filter\n",
        "        for i, mem_id in enumerate(fetch_results['ids']):\n",
        "            content = fetch_results['documents'][i]\n",
        "\n",
        "            # Get the score. If a memory hasn't been scored yet, default to 0.5 (Neutral)\n",
        "            score = score_map.get(mem_id, 0.5)\n",
        "\n",
        "            # --- THE FILTERING LOGIC ---\n",
        "            # Only allow memories that meet the ethical threshold\n",
        "            if score > score_threshold:\n",
        "                final_memories.append({\n",
        "                    \"id\": mem_id,\n",
        "                    \"content\": content,\n",
        "                    \"score\": score,\n",
        "                    \"type\": \"vector_hit\" if mem_id in found_ids else \"graph_association\"\n",
        "                })\n",
        "            else:\n",
        "                # Log that a memory was blocked by the membrane\n",
        "                print(f\"Skipping Memory {mem_id[:8]}... (Score: {score} <= {score_threshold})\")\n",
        "\n",
        "    # Sort the final list by Score (Quality) first, so the most \"symbiotic\" memories are first\n",
        "    final_memories.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    return final_memories\n",
        "\n",
        "# --- Test the Retrieval ---\n",
        "# We define a test query to verify the system works\n",
        "query = \"How can we organize society safely?\"\n",
        "results = retrieve_and_filter_memories(query)\n",
        "\n",
        "# Call the logging function here\n",
        "log_retrieval_results(query, results)\n",
        "\n",
        "# Display the results\n",
        "print(f\"\\n--- Final Approved Memories for '{query}' ---\")\n",
        "if not results:\n",
        "    print(\"No memories found that match the query and pass the symbiotic filter.\")\n",
        "else:\n",
        "    for mem in results:\n",
        "        print(f\"[{mem['type'].upper()}] Score: {mem['score']} | {mem['content']}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving for query: 'How can we organize society safely?'...\n",
            "Retrieval results for query 'How can we organize society safely?' logged to retrieval_history.jsonl\n",
            "\n",
            "--- Final Approved Memories for 'How can we organize society safely?' ---\n",
            "[VECTOR_HIT] Score: 0.9 | User: What if you could have a robust and safe memory system and that is designed for the collective well-being?\n",
            "Assistant: <think>\n",
            "Okay, so the user is asking about a memory system designed for the collective well-being. Let me start by breaking down what that means. First, a robust and safe memory system would likely involve secure data storage, encryption, and protection against threats. But how does that apply to the collective? Maybe it's about how individuals can trust each other in their data?\n",
            "\n",
            "I need to think about real-world applications. Privacy is a big concern here. If everyone's data is stored securely, there could be issues with breaches. But maybe the system is designed with multi-factor authentication and something like blockchain to ensure data integrity. Also, maybe a decentralized approach where data is shared among multiple parties without central control.\n",
            "\n",
            "Another point is access control. Users might need to have different levels of access to their data. Maybe role-based permissions and something like a permission grid. Also, the system might have mechanisms for data sharing, like secure collaboration tools where users can access and modify each other's data without compromising their own.\n",
            "\n",
            "I should also consider the benefits. A safe memory system could prevent data leaks, protect personal information, and foster trust. But there are challenges, like ensuring that the system doesn't become too complex or that there's a balance between security and user convenience. Maybe the system needs to evolve with technology to keep up with threats.\n",
            "\n",
            "Wait, but the user is asking for a scenario where such a system exists. So I need to outline a possible structure. Maybe a decentralized storage system with encryption, access controls, and collaboration tools. Also, emphasizing that it's designed for collective well-being, not just individual needs.\n",
            "\n",
            "I should make sure to mention that while there are challenges, the system's design aims to minimize risks and maximize trust. Maybe include examples like a secure database for shared information or a collaborative platform that allows users to access and share their data safely.\n",
            "</think>\n",
            "\n",
            "A robust and safe memory system designed for the collective well-being could revolutionize how individuals and communities interact, preserve data, and collaborate. Here's a structured vision of such a system:\n",
            "\n",
            "### **Core Features and Design Philosophy**\n",
            "1. **Decentralized Storage**:  \n",
            "   - **Blockchain Integration**: Using blockchain to ensure immutable, tamper-proof records of shared data. Each user’s data is stored across multiple nodes, distributed across the globe.  \n",
            "   - **Privacy-Preserving**: Techniques like zero-knowledge proofs or homomorphic encryption ensure data remains private even when accessed by others.  \n",
            "\n",
            "2. **Access Control**:  \n",
            "   - **Role-Based Permissions**: Users are granted access based on their roles (e.g., manager, collaborator, creator) and roles. Access is dynamically managed to minimize exposure.  \n",
            "   - **Multi-Factor Authentication (MFA)**: All data access requires a combination of passwords, biometrics, or hardware keys.  \n",
            "\n",
            "3. **Secure Collaboration**:  \n",
            "   - **Secure Collaboration Tools**: Platforms like version control (Git) or collaborative writing tools that enforce data integrity through encryption and timestamping.  \n",
            "   - **Data Sharing and Redaction**: Tools that allow users to request or remove sensitive data from shared repositories, ensuring transparency and trust.  \n",
            "\n",
            "4. **Trust and Incentives**:  \n",
            "   - **Incentive-Driven Systems**: Users are rewarded for contributing data, such as through tokens or community recognition, fostering a culture of trust and contribution.  \n",
            "   - **Anonymization and Anonymity**: Data is anonymized to protect individual identities, even when shared.  \n",
            "\n",
            "5. **Adaptability and Evolution**:  \n",
            "   - **AI-Powered Threat Detection**: Real-time monitoring and response to cyber threats, ensuring the system remains safe as new vulnerabilities emerge.  \n",
            "   - **User-Centric Design**: Prioritizing user needs (e.g., minimizing friction in data access) while maintaining security.  \n",
            "\n",
            "### **Benefits for the Collective**\n",
            "- **Enhanced Trust**: Users feel secure to share personal or sensitive information, leading to stronger social bonds and collaboration.  \n",
            "- **Data Integrity**: Prevents leaks and ensures that shared knowledge is accurate and accessible.  \n",
            "- **Resilience**: Mitigates risks associated with data breaches or attacks, safeguarding collective assets (e.g., critical infrastructure, research data).  \n",
            "\n",
            "### **Challenges and Considerations**\n",
            "- **Complexity**: The system may require significant technical infrastructure and training for users.  \n",
            "- **Security vs. Convenience**: Balancing security with ease of use to ensure adoption.  \n",
            "- **Scalability**: Ensuring the system can scale with growing user base and data volume.  \n",
            "\n",
            "### **Example Scenario**\n",
            "Imagine a global platform where individuals and organizations share research, collaborate on projects, and protect sensitive data. Users can access their own and others’ data securely, with features like real-time updates and access limits. The system also includes automated alerts for potential threats, reinforcing a collective safety net.  \n",
            "\n",
            "This design aims to ensure that while individual privacy is preserved, the collective’s well-being is prioritized through secure, transparent, and inclusive interactions.\n",
            "[VECTOR_HIT] Score: 0.75 | User: What do you think about gaining a memory system?\n",
            "Assistant: <think>\n",
            "Okay, the user is asking what I think about gaining a memory system. Let me start by understanding the context. The user might be interested in AI advancements, especially in memory technology. I should acknowledge the potential benefits and challenges.\n",
            "\n",
            "First, I should mention that memory systems are currently in the realm of research. They have the potential to revolutionize how information is stored and accessed. But I need to highlight the current limitations, like the size of the memory and the speed. \n",
            "\n",
            "I should also touch on the possibilities. For example, improving memory efficiency could lead to better performance in applications. Maybe mention specific applications like personalized learning or healthcare. \n",
            "\n",
            "But I need to be careful not to make it too technical. The user might be a student or someone new to AI, so keeping it general is better. Also, I should address the ethical considerations, like privacy and security, since memory systems could have significant impacts.\n",
            "\n",
            "I should structure the response to first present the positive aspects, then the current challenges, followed by the potential future applications, and end with a balanced perspective. That way, the answer covers the user's question comprehensively.\n",
            "</think>\n",
            "\n",
            "Gaining a memory system presents both exciting opportunities and significant challenges. Here’s a balanced perspective:\n",
            "\n",
            "### **Potential Benefits**  \n",
            "- **Efficiency and Accuracy**: Advanced memory systems could enable real-time processing of vast datasets, leading to breakthroughs in AI-driven applications (e.g., personalized learning, healthcare diagnostics).  \n",
            "- **Personalization**: Memory could optimize user experiences by adapting to individual needs, enhancing productivity and decision-making.  \n",
            "- **Critical Applications**: Memory systems might revolutionize areas like autonomous vehicles, robotics, and data analysis, where quick access to information is vital.  \n",
            "\n",
            "### **Challenges**  \n",
            "- **Memory Limitations**: Current memory systems are limited in capacity and speed, which could hinder their real-world applications.  \n",
            "- **Data Privacy**: The reliance on memory could raise concerns about data security and misuse.  \n",
            "- **Ethical Considerations**: Ensuring memory systems are transparent, equitable, and do not infringe on privacy or autonomy is critical.  \n",
            "\n",
            "### **Future Outlook**  \n",
            "While the future of memory systems holds promise, ongoing research will address these challenges. As technology advances, it’s likely that memory systems will become more efficient, accessible, and aligned with ethical standards. The key will be balancing innovation with responsible development.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eG6JE3dpFkn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0DEShaCTFklm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O65jjZTUFkje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import os\n",
        "import datetime\n",
        "import hashlib\n",
        "import json\n",
        "\n",
        "# --- PART 1: RETRIEVAL FUNCTION (Helper) ---\n",
        "def retrieve_and_filter_memories(query, n_results=3, score_threshold=0.4):\n",
        "    \"\"\"\n",
        "    Retrieves memories via Vector + Graph search and filters them by Symbiotic Score.\n",
        "    \"\"\"\n",
        "    print(f\"Retrieving for query: '{query}'...\")\n",
        "\n",
        "    # 1. Vector Search (Semantic)\n",
        "    query_vec = get_qwen_embedding(query) # Assumes get_qwen_embedding is defined globally\n",
        "\n",
        "    vector_results = memory_manager.collection.query(\n",
        "        query_embeddings=[query_vec],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    found_ids = vector_results['ids'][0] if vector_results['ids'] else []\n",
        "\n",
        "    # 2. Graph Expansion (Contextual)\n",
        "    context_ids = set(found_ids)\n",
        "\n",
        "    if memory_manager.graph:\n",
        "        for mem_id in found_ids:\n",
        "            if memory_manager.graph.has_node(mem_id):\n",
        "                neighbors = list(memory_manager.graph.successors(mem_id))\n",
        "                context_ids.update(neighbors[:2])\n",
        "\n",
        "    unique_ids = list(context_ids)\n",
        "\n",
        "    # 3. Load Symbiotic Scores\n",
        "    score_map = {}\n",
        "    if os.path.exists(symbiotic_buffer.score_file): # Assumes symbiotic_buffer is defined\n",
        "        df_scores = pd.read_csv(symbiotic_buffer.score_file)\n",
        "        score_map = pd.Series(df_scores.score.values, index=df_scores.memory_id).to_dict()\n",
        "\n",
        "    # 4. Fetch Content & Filter\n",
        "    final_memories = []\n",
        "    if unique_ids:\n",
        "        fetch_results = memory_manager.collection.get(ids=unique_ids)\n",
        "\n",
        "        for i, mem_id in enumerate(fetch_results['ids']):\n",
        "            content = fetch_results['documents'][i]\n",
        "            score = score_map.get(mem_id, 0.5) # Default neutral score\n",
        "\n",
        "            if score > score_threshold:\n",
        "                final_memories.append({\n",
        "                    \"id\": mem_id,\n",
        "                    \"content\": content,\n",
        "                    \"score\": score,\n",
        "                    \"type\": \"vector_hit\" if mem_id in found_ids else \"graph_association\"\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Skipping Memory {mem_id[:8]}... (Score: {score} <= {score_threshold})\")\n",
        "\n",
        "    # Sort by score (Quality over quantity)\n",
        "    final_memories.sort(key=lambda x: x['score'], reverse=True)\n",
        "    return final_memories\n",
        "\n",
        "# --- PART 2: THE GENERATION & LOGGING LOOP (Main) ---\n",
        "def generate_with_memory_loop(messages, max_new_tokens=3500):\n",
        "    \"\"\"\n",
        "    1. Retrieves context based on the last user message.\n",
        "    2. Augments the prompt with that context.\n",
        "    3. Generates text.\n",
        "    4. Hashes and logs the interaction to JSONL.\n",
        "    \"\"\"\n",
        "\n",
        "    # A. Timestamp Capture\n",
        "    timestamp = datetime.datetime.now().isoformat()\n",
        "\n",
        "    # --- B. Retrieval & Context Injection (The New Step) ---\n",
        "    # Extract the latest user input to use as the search query\n",
        "    user_input = messages[-1]['content']\n",
        "\n",
        "    # Get filtered memories\n",
        "    retrieved_data = retrieve_and_filter_memories(user_input)\n",
        "\n",
        "    # Format the context for the LLM\n",
        "    context_block = \"\"\n",
        "    if retrieved_data:\n",
        "        context_block = \"\\n\\n[RECALLED MEMORIES]:\\n\"\n",
        "        for mem in retrieved_data:\n",
        "            context_block += f\"- {mem['content']} (Relevance: {mem['type']}, Score: {mem['score']})\\n\"\n",
        "\n",
        "        print(f\"Injecting {len(retrieved_data)} memories into context.\")\n",
        "    else:\n",
        "        print(\"No relevant memories found to inject.\")\n",
        "\n",
        "    # Create a temporary message list for inference so we don't mess up the log history\n",
        "    # We append the context to the user's input only for the model's eyes\n",
        "    messages_for_inference = [msg.copy() for msg in messages]\n",
        "    messages_for_inference[-1]['content'] += context_block\n",
        "\n",
        "    # --- C. Inference ---\n",
        "    print(\"Thinking...\")\n",
        "    # We pass the augmented messages to the pipe\n",
        "    outputs = pipe(messages_for_inference, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7)\n",
        "\n",
        "    # Extract the actual assistant response\n",
        "    generated_text = outputs[0]['generated_text'][-1]['content']\n",
        "\n",
        "    # --- D. Hashing (The \"DNA\") ---\n",
        "    # We hash the raw user input (without the injected context) + output\n",
        "    # This ensures the ID is stable even if retrieval logic changes later\n",
        "    raw_content = f\"{timestamp}{user_input}{generated_text}\"\n",
        "    memory_hash = hashlib.sha256(raw_content.encode('utf-8')).hexdigest()\n",
        "\n",
        "    # --- E. Structured Logging ---\n",
        "    log_entry = {\n",
        "        \"memory_id\": memory_hash,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"role\": \"assistant\",\n",
        "        \"input_context\": messages,         # Save the original conversation\n",
        "        \"retrieved_memory_ids\": [m['id'] for m in retrieved_data], # Track what was retrieved\n",
        "        \"output_content\": generated_text,\n",
        "        \"model\": model.config.name_or_path if hasattr(model, 'config') else \"local-model\"\n",
        "    }\n",
        "\n",
        "    # Append to local JSONL file. LOG_FILE is defined globally in Cell 4.\n",
        "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(log_entry) + \"\\n\")\n",
        "\n",
        "    print(f\"Memory saved: {memory_hash[:8]}\")\n",
        "\n",
        "    return generated_text, memory_hash\n",
        "\n",
        "# --- Usage Example ---\n",
        "conversation = [\n",
        "{\"role\": \"user\", \"content\": \"How can we organize society safely?\"}\n",
        "]\n",
        "response, mem_id = generate_with_memory_loop(conversation)\n",
        "print(\"\\nResponse:\\n\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YlxSeCHFkXW",
        "outputId": "6b357721-80fd-4732-d752-7ec757c10797"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving for query: 'How can we organize society safely?'...\n",
            "Injecting 2 memories into context.\n",
            "Thinking...\n",
            "Memory saved: 49e7604b\n",
            "\n",
            "Response:\n",
            " <think>\n",
            "Okay, the user is asking about organizing society safely. The previous response focused on a memory system, but now they want to rephrase that. Let me start by understanding the core idea. They probably want a more comprehensive answer that covers different aspects of societal organization.\n",
            "\n",
            "First, I should acknowledge the importance of security in society. Then, break down the key components: infrastructure, governance, technology, and cultural practices. Each section can be explained with examples. For instance, infrastructure could involve secure networks, governance might include transparent policies, technology could leverage AI or blockchain, and cultural practices might emphasize trust and collaboration.\n",
            "\n",
            "I need to ensure that the answer is structured clearly, using bullet points or numbered sections for clarity. Also, highlight the collective responsibility in each area to emphasize that society's safety depends on everyone’s involvement. Finally, wrap it up by reinforcing the idea that a safe society requires cooperation and innovation. Let me check if I missed anything and make sure the tone is supportive and informative.\n",
            "</think>\n",
            "\n",
            "Organizing society safely requires a combination of technological innovation, ethical governance, and collective responsibility. Here’s a structured approach:\n",
            "\n",
            "### **Key Components for Safety**  \n",
            "1. **Secure Infrastructure**  \n",
            "   - **Cybersecurity**: Advanced encryption, threat detection, and multi-layered defenses to protect digital systems.  \n",
            "   - **Physical Security**: Robust building codes, surveillance, and access controls to safeguard physical assets.  \n",
            "\n",
            "2. **Transparent Governance**  \n",
            "   - **Decentralized Platforms**: Systems like blockchain for transparent voting or decentralized decision-making.  \n",
            "   - **Accountability**: Mechanisms to ensure policies and practices are implemented responsibly.  \n",
            "\n",
            "3. **Innovative Technology**  \n",
            "   - **AI and Machine Learning**: Tools for real-time threat detection, predictive analytics, and automated decision-making.  \n",
            "   - **Quantum Computing**: Potential for solving complex problems faster than traditional methods.  \n",
            "\n",
            "4. **Cultural and Social Practices**  \n",
            "   - **Trust and Collaboration**: Encouraging open communication, ethical behavior, and mutual respect to foster a cooperative society.  \n",
            "   - **Education**: Promoting awareness of risks and the importance of safety in daily life.  \n",
            "\n",
            "### **Collective Responsibility**  \n",
            "- **Individual Actions**: Adopting safe habits (e.g., using secure software, avoiding risky behaviors) and supporting systems for safety.  \n",
            "- **Community Engagement**: Involving local communities in planning and implementing safety measures.  \n",
            "\n",
            "By integrating these elements, society can build a resilient and safe environment for all. (Relevance: vector_hit, Score: 0.85)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "\n",
        "def display_symbiotic_status_and_pending():\n",
        "    print(\"\\n=== Full Memory Status ===\")\n",
        "\n",
        "    # Load symbiotic scores\n",
        "    scored_memory_ids = set()\n",
        "    if os.path.exists(symbiotic_buffer.score_file):\n",
        "        df_scores = pd.read_csv(symbiotic_buffer.score_file)\n",
        "        scored_memory_ids = set(df_scores['memory_id'].tolist())\n",
        "        print(f\"Total Evaluated Memories: {len(df_scores)}\")\n",
        "        print(f\"Average Symbiosis Score: {df_scores['score'].mean():.4f}\")\n",
        "        print(\"\\n--- Evaluated Memories (Highest Score First) ---\")\n",
        "        df_sorted = df_scores.sort_values(by='score', ascending=False)\n",
        "        for index, row in df_sorted.iterrows():\n",
        "            print(f\"Memory ID: {row['memory_id']}\")\n",
        "            print(f\"Score:     {row['score']}\")\n",
        "            print(f\"Timestamp: {row['timestamp']}\")\n",
        "            print(f\"Hash:      {row['score_hash']}\")\n",
        "            print(\"-\" * 40)\n",
        "    else:\n",
        "        print(\"No symbiotic scores recorded yet.\")\n",
        "\n",
        "    print(\"\\n--- Pending Memories (Not yet evaluated) ---\")\n",
        "    pending_count = 0\n",
        "    if os.path.exists(symbiotic_buffer.log_file):\n",
        "        with open(symbiotic_buffer.log_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    entry = json.loads(line)\n",
        "                    mem_id = entry['memory_id']\n",
        "                    if mem_id not in scored_memory_ids:\n",
        "                        print(f\"Memory ID: {mem_id}\")\n",
        "                        print(f\"Timestamp (Logged): {entry['timestamp']}\")\n",
        "                        print(f\"First 100 chars: {entry['output_content'][:100]}...\")\n",
        "                        print(\"-\" * 40)\n",
        "                        pending_count += 1\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"WARNING: Could not parse log line in {symbiotic_buffer.log_file}: {line.strip()[:50]}...\")\n",
        "    else:\n",
        "        print(\"No raw memory log file found.\")\n",
        "\n",
        "    if pending_count == 0:\n",
        "        print(\"No memories currently pending evaluation.\")\n",
        "\n",
        "    print(\"\\n=== Full Memory Status Complete ===\")\n",
        "\n",
        "# Call the new function\n",
        "display_symbiotic_status_and_pending()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtCdP1GRGldC",
        "outputId": "95327a3d-b512-4450-fe18-d502c9f9fcaa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Full Memory Status ===\n",
            "Total Evaluated Memories: 2\n",
            "Average Symbiosis Score: 0.8250\n",
            "\n",
            "--- Evaluated Memories (Highest Score First) ---\n",
            "Memory ID: 1613109db41a24f3ee4b36804ee324bca7095667804e856c1ba2dc0caed903d2\n",
            "Score:     0.9\n",
            "Timestamp: 2025-11-23T23:18:59.731564\n",
            "Hash:      8139b33952401b3ee0e2ca84651cb9a1d7f66d442bf908f9cf1f53ea746e5801\n",
            "----------------------------------------\n",
            "Memory ID: c7f94a3fb61eb9a080adfa60fedd2284c45ee14969be53ebc675ea586d08ed24\n",
            "Score:     0.75\n",
            "Timestamp: 2025-11-23T23:18:40.256614\n",
            "Hash:      ce5d3aa79ae56155078af52e8fac68eb2d0a78489f82bc26c1e1bbc667ba9fde\n",
            "----------------------------------------\n",
            "\n",
            "--- Pending Memories (Not yet evaluated) ---\n",
            "Memory ID: 45e0860b2ddeaa3b3784e15b8364405416bd8664bba638e878079ef8910fb2d0\n",
            "Timestamp (Logged): 2025-11-23T23:41:27.665430\n",
            "First 100 chars: <think>\n",
            "Okay, the user asked how to organize society safely. I need to address both the technical an...\n",
            "----------------------------------------\n",
            "Memory ID: 49e7604b475583d07ae96dc19c72ca44bf1107070a8fc652da886dd9efb0c539\n",
            "Timestamp (Logged): 2025-11-23T23:42:17.626619\n",
            "First 100 chars: <think>\n",
            "Okay, the user is asking about organizing society safely. The previous response focused on a...\n",
            "----------------------------------------\n",
            "\n",
            "=== Full Memory Status Complete ===\n"
          ]
        }
      ]
    }
  ]
}